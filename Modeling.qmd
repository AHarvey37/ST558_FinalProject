---
title: "Modeling"
author: "Andrew Harvey"
format: html
editor: visual
---

```{r Libraries and Raw data, warning=FALSE, message=FALSE,}
library(tidyverse)
library(caret)
library(tree)
```


```{r Read in Data and Set Factors}
rawData <- read.csv("./diabetes_binary_health_indicators_BRFSS2015.csv")

#create a new data frame for cleaning and converts all binary variables into factors
cleaned<- rawData|>
  select(Diabetes_binary,HighBP,HighChol,HvyAlcoholConsump,Smoker,PhysActivity,Age,HeartDiseaseorAttack,MentHlth,BMI)|>
  mutate(Diabetes_binary = factor(Diabetes_binary,labels = c("No","Yes")),
         Age = as.factor(Age),
         HighBP = as.factor(HighBP),
         HighChol = as.factor(HighChol),
         HvyAlcoholConsump = as.factor(HvyAlcoholConsump),
         Smoker = as.factor(Smoker),
         PhysActivity = as.factor(PhysActivity),
         HeartDiseaseorAttack=as.factor(HeartDiseaseorAttack),
         MentHlth=as.factor(MentHlth))

```


## Modeling



```{r Split Data}
# set seed for predictability
set.seed(8)

# Create a Vector to use to split data. Used createdatapartition to help maintain the ratio of diabates positive to diabetes negative
trainingVec <- createDataPartition(cleaned$Diabetes_binary,
                                   p = .7,
                                   list = FALSE)

# Split data into training and test sets
## Training set
diabetesTrain <- cleaned[trainingVec,]
## Test set
diabetesTest <- cleaned[-trainingVec,]

```


# write a paragraph here about log loss
mnLogLoss computes the minus log-likelihood of the multinomial distribution (without the constant term): $$-logLoss = \frac{-1}{n}\sum_{i=1}^{n}\sum_{j=1}^{C}y_{ij}log(p_{ij})$$

```{r set train control and 5 fold cross-validation using mnLogLoss}
# make Train Control Variable with 5 fold cross-validation
trctrl<- trainControl(method = "cv",
                      number = 5,
                      classProbs = TRUE,
                      summaryFunction = mnLogLoss)
```

# write a paragraph about log models
lower the logloss the better, it is used for classification models in machine learning perfect logloss = 0. logLoss calculates the actual value of an observation and finds the difference between that actual value and the predicted value. The total logloss for a model is the mean logloss for the model. 

# Write a paragraph about logistic regression models and why use

```{r train and fit glm}
# Generalized Linear Model
Diabetes_logFit1<-train(Diabetes_binary ~ HighBP+HighChol+BMI+HvyAlcoholConsump+Smoker,
                       data = diabetesTrain,
                       method = "glm",
                       trControl = trctrl,
                       metric = "logLoss",
                       family = "binomial")

# Generalized Linear Model
Diabetes_logFit2<-train(Diabetes_binary ~ HighBP+HighChol+BMI+HvyAlcoholConsump+Smoker+PhysActivity+Age,
                       data = diabetesTrain,
                       method = "glm",
                       trControl = trctrl,
                       metric = "logLoss",
                       family = "binomial")

# Generalized Linear Model
Diabetes_logFit3<-train(Diabetes_binary ~ .,
                       data = diabetesTrain,
                       method = "glm",
                       trControl = trctrl,
                       metric = "logLoss",
                       family = "binomial",
                       #tuneGrid = expand.grid(nIter = 3)
                       )

# Create a Comparison Table
#Diabetes_logFit3$results<-rename(Diabetes_logFit3$results, parameter = nIter)

Comparison <- data.frame(rbind(Diabetes_logFit1$results,Diabetes_logFit2$results,Diabetes_logFit3$results))|>
  mutate(Model_type = c("1","2","3"))|>
  select(Model_type,logLoss,logLossSD)

Comparison

# Best model = BayesGLM
```

# write paragraph on Classigication trees

```{r Classification trees}
# classTree_Fit<- tree(Diabetes_binary ~                           Age+HighChol+HighBP+HvyAlcoholConsump+Smoker+PhysActivity,
#                      data = diabetesTrain,
#                      split = "deviance"
#                      )
# # summary(classTree_Fit)
# # plot(classTree_Fit)
# classTree_Predict<-predict(classTree_Fit,
#                            newdata = diabetesTest
#                            )

cart_TreeFit<- train(Diabetes_binary ~ HighBP+HighChol+BMI+HvyAlcoholConsump+Smoker,
                       data = diabetesTrain,
                       method = "rpart",
                       trControl = trctrl,
                       metric = "logLoss",
                       #family = "binomial",
                       tuneGrid = expand.grid(cp = seq(0,1,by=.1))
                       )
cart_TreePredict <- predict(cart_TreeFit,
                            newdata = diabetesTest,
                            type = "prob")

cart_TreeFit2<- train(Diabetes_binary ~ HighBP+HighChol+BMI+HvyAlcoholConsump+Smoker+PhysActivity+Age,
                       data = diabetesTrain,
                       method = "rpart",
                       trControl = trctrl,
                       metric = "logLoss",
                       #family = "binomial",
                       tuneGrid = expand.grid(cp = seq(0,1,by=.1))
                       )
cart_TreePredict2 <- predict(cart_TreeFit2,
                            newdata = diabetesTest,
                            type = "prob")

cart_TreeFit3<- train(Diabetes_binary ~ .,
                       data = diabetesTrain,
                       method = "rpart",
                       trControl = trctrl,
                       metric = "logLoss",
                       #family = "binomial",
                       tuneGrid = expand.grid(cp = seq(0,1,by=.1))
                       )
cart_TreePredict3 <- predict(cart_TreeFit3,
                            newdata = diabetesTest,
                            type = "prob")


cart_TreeFit$bestTune
cart_TreeFit2$bestTune
cart_TreeFit3$bestTune

Comparison_tree<- data.frame(rbind(cart_TreeFit$results,
                                   cart_TreeFit2$results,
                                   cart_TreeFit3$results))|>
  mutate(Model_type = c("1","1","1","1","1","1","1","1","1","1","1","2","2","2","2","2","2","2","2","2","2","2","3","3","3","3","3","3","3","3","3","3","3"))|>
  select(Model_type,cp,logLoss,logLossSD)

Comparison_tree

# Best Tree = rpart w/ cp =0.0

```



# random forrest explain random forests

```{r random forests}
rf_Fit<- train(Diabetes_binary ~ HighBP+HighChol+BMI+HvyAlcoholConsump+Smoker,
               data = diabetesTrain,
               method = "ranger",
               trControl = trctrl,
               metric = "logLoss",
               tuneGrid = expand.grid(mtry = 3,
                                      splitrule = "extratrees",
                                      min.node.size = 100)
                       )

rf_Pred<- predict(rf_Fit,
                  newdata = diabetesTest,
                  type = "prob")

rf_Fit2<- train(Diabetes_binary ~ HighBP+HighChol+BMI+HvyAlcoholConsump+Smoker+PhysActivity+Age,
                data = diabetesTrain,
                method = "ranger",
                trControl = trctrl,
                metric = "logLoss",
                tuneGrid = expand.grid(mtry = 3,
                                       splitrule = "extratrees",
                                       min.node.size = 100)
                       )

rf_Pred2<- predict(rf_Fit2,
                  newdata = diabetesTest,
                  type = "prob")

rf_Fit3<- train(Diabetes_binary ~ .,
                data = diabetesTrain,
                method = "ranger",
                trControl = trctrl,
                metric = "logLoss",
                tuneGrid = expand.grid(mtry = 3,
                                       splitrule = "extratrees",
                                       min.node.size = 100)
                       )

rf_Pred3<- predict(rf_Fit3,
                  newdata = diabetesTest,
                  type = "prob")


rf_Fit$results
rf_Fit2$results
rf_Fit3$results

Comparison_forest<- data.frame(
  rbind(
    rf_Fit$results[,c("logLoss","logLossSD")],
    rf_Fit2$results[,c("logLoss","logLossSD")],
    rf_Fit3$results[,c("logLoss","logLossSD")])|>
  mutate(Model_type = c("1","2","3"))|>
  select(Model_type,logLoss,logLossSD))

Comparison_forest

# Best Forest = range
```

# final model selection

```{r compare best models}

bayesGLMPredict<- predict(Diabetes_logFit2,
                          newdata = diabetesTest,
                          type = "prob")
#bayesGLMPredict

cart_TreePredict<-predict(cart_TreeFit,
                          newdata = diabetesTest,
                          type = "prob")

#cart_TreePredict

rf_Pred<-predict(rf_Fit,
                 newdata = diabetesTest,
                 type = "prob")

#rf_Pred

Logloss<- function(real,prediction){
  results<- -1/length(real)*(sum((real*log(prediction)+(1-real)*log(1-prediction))))
  return(results)
}

final_compar<-data.frame(rbind(Logloss(rawData$Diabetes_binary,bayesGLMPredict),
                               Logloss(rawData$Diabetes_binary,cart_TreePredict),
                               Logloss(rawData$Diabetes_binary,rf_Pred)))|>
  mutate(BestModel = c("Bayes GLM", "Classification Tree", "Random Forest"))

names(final_compar)<- c("LogLoss","Mehod")
final_compar

# best model = Classification tree with all variables
```

