---
title: "Modeling"
author: "Andrew Harvey"
format: html
editor: visual
---

```{r Libraries and Raw data, warning=FALSE, message=FALSE,}
library(tidyverse)
library(caret)
library(tree)
```


```{r Read in Data and Set Factors}
rawData <- read.csv("./diabetes_binary_health_indicators_BRFSS2015.csv")

#create a new data frame for cleaning and converts all binary variables into factors
cleaned<- rawData|>
  select(Diabetes_binary,Age,Education,Income,GenHlth)|>
  mutate(Diabetes_binary = factor(Diabetes_binary,labels = c("No","Yes")),
         Age = as.factor(Age),
         Education = as.factor(Education),
         Income = as.factor(Income),
         GenHlth = as.factor(GenHlth))

```


# ```{r Create dummy vars}
# dummies<- dummyVars(~.,data = cleaned,sep=".")
# cleaned_wDummies<-predict(dummies,newdata = cleaned)|>
#   as_tibble()|>
#   mutate(Diabetes_binary = factor(Diabetes_binary,labels = c("No","Yes")))
# ```

## Modeling

chosen will be age education and income

```{r Split Data}
# set seed for predictability
set.seed(8)

# Create a Vector to use to split data. Used createdatapartition to help maintain the ratio of diabates positive to diabetes negative
trainingVec <- createDataPartition(cleaned$Diabetes_binary,
                                   p = .7,
                                   list = FALSE)

# Split data into training and test sets
## Training set
diabetesTrain <- cleaned[trainingVec,]
## Test set
diabetesTest <- cleaned[-trainingVec,]

```


# write a paragraph here about log loss
mnLogLoss computes the minus log-likelihood of the multinomial distribution (without the constant term): $$-logLoss = \frac{-1}{n}\sum_{i=1}^{n}\sum_{j=1}^{C}y_{ij}log(p_{ij})$$

```{r set train control and 5 fold cross-validation using mnLogLoss}
# make Train Control Variable with 5 fold cross-validation
trctrl<- trainControl(method = "cv",
                      number = 5,
                      classProbs = TRUE,
                      summaryFunction = mnLogLoss)
```

# write a paragraph about log models
lower the logloss the better, it is used for classification models in machine learning perfect logloss = 0. logLoss calculates the actual value of an observation and finds the difference between that actual value and the predicted value. The total logloss for a model is the mean logloss for the model. 

# Write a paragraph about logistic regression models and why use

```{r train and fit glm}
# Generalized Linear Model
Diabetes_logFit1<-train(Diabetes_binary ~ Age+Income+Education+GenHlth,
                       data = diabetesTrain,
                       method = "glm",
                       trControl = trctrl,
                       metric = "logLoss",
                       family = "binomial")

# Bayes Generalized Linear Model
Diabetes_logFit2<-train(Diabetes_binary ~ Age+Income+Education+GenHlth,
                       data = diabetesTrain,
                       method = "bayesglm",
                       trControl = trctrl,
                       metric = "logLoss",
                       family = "binomial")

# Boosted Logistic Regression
Diabetes_logFit3<-train(Diabetes_binary ~ Age+Income+Education+GenHlth,
                       data = diabetesTrain,
                       method = "LogitBoost",
                       trControl = trctrl,
                       metric = "logLoss",
                       family = "binomial",
                       tuneGrid = expand.grid(nIter = 3)
                       )

# Create a Comparison Table
Diabetes_logFit3$results<-rename(Diabetes_logFit3$results, parameter = nIter)

Comparison <- data.frame(rbind(Diabetes_logFit1$results,Diabetes_logFit2$results,Diabetes_logFit3$results))|>
  mutate(Model_type = c("GLM","BayesGLM","LogitBoost"))|>
  select(Model_type,parameter,logLoss,logLossSD)

Comparison
```

# write paragraph on Classigication trees

```{r Classification trees}
classTree_Fit<- tree(Diabetes_binary ~ Age+Income+Education+GenHlth,
                     data = diabetesTrain,
                     split = "deviance"
                     )
# summary(classTree_Fit)
# plot(classTree_Fit)
classTree_Predict<-predict(classTree_Fit,
                           newdata = diabetesTest
                           )

cart_TreeFit<- train(Diabetes_binary ~ Age+Income+Education+GenHlth,
                       data = diabetesTrain,
                       method = "rpart",
                       trControl = trctrl,
                       metric = "logLoss",
                       #family = "binomial",
                       tuneGrid = expand.grid(cp = seq(0,1,by=.01))
                       )
cart_TreePredict <- predict(cart_TreeFit,
                            newdata = diabetesTest,
                            type = "prob")
cart_TreePredict
```



# random forrest

```{r}
rf_Fit<- train(Diabetes_binary ~ Age+Income+Education+GenHlth,
                       data = diabetesTrain,
                       method = "ranger",
                       trControl = trctrl,
                       metric = "logLoss",
                       #family = "binomial",
                       tuneGrid = expand.grid(mtry = 3,
                                              splitrule = "extratrees",
                                              min.node.size = 100)
                       )
```

