---
title: "Modeling"
author: "Andrew Harvey"
format: html
editor: visual
---

```{r Libraries and Raw data, warning=FALSE, message=FALSE,}
library(tidyverse)
library(caret)
library(tree)
```


```{r Read in Data and Set Factors}
rawData <- read.csv("./diabetes_binary_health_indicators_BRFSS2015.csv")

#create a new data frame for cleaning and converts all binary variables into factors
cleaned<- rawData|>
  select(Diabetes_binary,Age,Education,Income,GenHlth)|>
  mutate(Diabetes_binary = factor(Diabetes_binary,labels = c("No","Yes")),
         Age = as.factor(Age),
         Education = as.factor(Education),
         Income = as.factor(Income),
         GenHlth = as.factor(GenHlth))

```


# ```{r Create dummy vars}
# dummies<- dummyVars(~.,data = cleaned,sep=".")
# cleaned_wDummies<-predict(dummies,newdata = cleaned)|>
#   as_tibble()|>
#   mutate(Diabetes_binary = factor(Diabetes_binary,labels = c("No","Yes")))
# ```

## Modeling

chosen will be age education and income

```{r Split Data}
# set seed for predictability
set.seed(8)

# Create a Vector to use to split data. Used createdatapartition to help maintain the ratio of diabates positive to diabetes negative
trainingVec <- createDataPartition(cleaned$Diabetes_binary,
                                   p = .7,
                                   list = FALSE)

# Split data into training and test sets
## Training set
diabetesTrain <- cleaned[trainingVec,]
## Test set
diabetesTest <- cleaned[-trainingVec,]

```


# write a paragraph here about log loss
mnLogLoss computes the minus log-likelihood of the multinomial distribution (without the constant term): $$-logLoss = \frac{-1}{n}\sum_{i=1}^{n}\sum_{j=1}^{C}y_{ij}log(p_{ij})$$

```{r set train control and 5 fold cross-validation using mnLogLoss}
# make Train Control Variable with 5 fold cross-validation
trctrl<- trainControl(method = "cv",
                      number = 5,
                      classProbs = TRUE,
                      summaryFunction = mnLogLoss)
```

# write a paragraph about log models
lower the logloss the better, it is used for classification models in machine learning perfect logloss = 0. logLoss calculates the actual value of an observation and finds the difference between that actual value and the predicted value. The total logloss for a model is the mean logloss for the model. 

# Write a paragraph about logistic regression models and why use

```{r train and fit glm}
# Generalized Linear Model
Diabetes_logFit1<-train(Diabetes_binary ~ Age+Income+Education+GenHlth,
                       data = diabetesTrain,
                       method = "glm",
                       trControl = trctrl,
                       metric = "logLoss",
                       family = "binomial")

# Bayes Generalized Linear Model
Diabetes_logFit2<-train(Diabetes_binary ~ Age+Income+Education+GenHlth,
                       data = diabetesTrain,
                       method = "bayesglm",
                       trControl = trctrl,
                       metric = "logLoss",
                       family = "binomial")

# Boosted Logistic Regression
Diabetes_logFit3<-train(Diabetes_binary ~ Age+Income+Education+GenHlth,
                       data = diabetesTrain,
                       method = "LogitBoost",
                       trControl = trctrl,
                       metric = "logLoss",
                       family = "binomial",
                       tuneGrid = expand.grid(nIter = 3)
                       )

# Create a Comparison Table
Diabetes_logFit3$results<-rename(Diabetes_logFit3$results, parameter = nIter)

Comparison <- data.frame(rbind(Diabetes_logFit1$results,Diabetes_logFit2$results,Diabetes_logFit3$results))|>
  mutate(Model_type = c("GLM","BayesGLM","LogitBoost"))|>
  select(Model_type,parameter,logLoss,logLossSD)

Comparison

# Best model = BayesGLM
```

# write paragraph on Classigication trees

```{r Classification trees}
# classTree_Fit<- tree(Diabetes_binary ~ Age+Income+Education+GenHlth,
#                      data = diabetesTrain,
#                      split = "deviance"
#                      )
# # summary(classTree_Fit)
# # plot(classTree_Fit)
# classTree_Predict<-predict(classTree_Fit,
#                            newdata = diabetesTest
#                            )

cart_TreeFit<- train(Diabetes_binary ~ Age+Income+Education+GenHlth,
                       data = diabetesTrain,
                       method = "rpart",
                       trControl = trctrl,
                       metric = "logLoss",
                       #family = "binomial",
                       tuneGrid = expand.grid(cp = seq(0,1,by=.1))
                       )
cart_TreePredict <- predict(cart_TreeFit,
                            newdata = diabetesTest,
                            type = "prob")

cart_TreeFit2<- train(Diabetes_binary ~ Age+Income+Education+GenHlth,
                       data = diabetesTrain,
                       method = "ctree",
                       trControl = trctrl,
                       metric = "logLoss",
                       #family = "binomial",
                       tuneGrid = expand.grid(mincriterion = 3)
                       )
cart_TreePredict2 <- predict(cart_TreeFit2,
                            newdata = diabetesTest,
                            type = "prob")

cart_TreeFit3<- train(Diabetes_binary ~ Age+Income+Education+GenHlth,
                       data = diabetesTrain,
                       method = "ctree2",
                       trControl = trctrl,
                       metric = "logLoss",
                       #family = "binomial",
                       tuneGrid = expand.grid(mincriterion = 3,
                                              maxdepth = 5)
                       )
cart_TreePredict3 <- predict(cart_TreeFit3,
                            newdata = diabetesTest,
                            type = "prob")


cart_TreeFit$results[,c("logLoss","logLossSD")]
cart_TreeFit2$results[,2:3]
cart_TreeFit3$results[,3:4]

Comparison_tree<- data.frame(rbind(cart_TreeFit$results[1:3,c("logLoss","logLossSD")],
                                   cart_TreeFit2$results[,2:3],
                                   cart_TreeFit3$results[,3:4]))|>
  mutate(Model_type = c("rpart","rpart","rpart","ctree","ctree2"))|>
  select(Model_type,logLoss,logLossSD)

Comparison_tree

# Best Tree = rpart w/ cp =0.0

```



# random forrest explain random forests

```{r random forests}
rf_Fit<- train(Diabetes_binary ~ Age+Income+Education+GenHlth,
                       data = diabetesTrain,
                       method = "ranger",
                       trControl = trctrl,
                       metric = "logLoss",
                       #family = "binomial",
                       tuneGrid = expand.grid(mtry = 3,
                                              splitrule = "extratrees",
                                              min.node.size = 100)
                       )

rf_Pred<- predict(rf_Fit,
                  newdata = diabetesTest,
                  type = "prob")

rf_Fit2<- train(Diabetes_binary ~ Age+Income+Education+GenHlth,
                       data = diabetesTrain,
                       method = "Rborist",
                       trControl = trctrl,
                       metric = "logLoss",
                       #family = "binomial",
                       tuneGrid = expand.grid(predFixed = 2,
                                              minNode = 5)
                       )

rf_Pred2<- predict(rf_Fit2,
                  newdata = diabetesTest,
                  type = "prob")

rf_Fit3<- train(Diabetes_binary ~ Age+Income+Education+GenHlth,
                       data = diabetesTrain,
                       method = "qrf",
                       trControl = trctrl,
                       metric = "logLoss",
                       #family = "binomial",
                       tuneGrid = expand.grid(predFixed = 2,
                                              minNode = 5)
                       )

rf_Pred3<- predict(rf_Fit3,
                  newdata = diabetesTest,
                  type = "prob")


rf_Fit$results
rf_Fit2$bestTune

```

