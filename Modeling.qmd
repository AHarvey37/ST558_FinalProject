---
title: "Modeling"
author: "Andrew Harvey"
format: html
editor: visual
---

# Modeling File

## Introduction Section

|   This project will use the "diabetes_binary_health_indicators_BRFSS2015" data set. According [Kaggle.com](https://www.kaggle.com/datasets/alexteboul/diabetes-health-indicators-dataset/data) this data set contains 253,680 observation where the response variable is whether the observed individual does or does not not have diabetes (0 or 1 respectively) or has pre-diabetes/diabetes (1). The original data set contains 22 variables most of which are categorical. We have limited the variables to those below:

1.  Diabetes_binary (response variable) - This variable illustrates if the subject has or does not have diabetes. 
2.  HighBP (Categorical) - This is a binary variable describing whether or not the subject has high blood pressure.
3.  HighChol (Categorical) - This is a binary variable describing whether or not the subject has high cholesterol.
4.  Smoker (Categorical) - This is a binary variable describing whether or not the subject has smoked at least 100 cigarettes in their lifetime. 100 cigarettes is equal to 5 packs.
5.  HeartDiseaseorAttack (Categorical) - This is a binary variable describing whether or not the subject has coronary heart disease or myocardial infarction (a heart attack).  
6.  PhysActivity (Categorical) - This is a binary variable describing whether or not the subject has conducted physical activity (outside of their job) in the past 30 days.
7.  HvyAlcoholConsump (Categorical) - This is a binary variable describing whether or not the subject, if male, consumes 14 or more alcoholic drinks per week, or if female, consumes 7 or more alcoholic drinks per week. 
8.  MentHlth - This variable is a scale from 1 to 30 of how many days the subject has had POOR mental health.
9.  Age (Categorical) - This variable breaks age into 13 levels which are 18-24, 25-29, 30-34, 35-39, 40-44, 45-49, 50-54, 55-59, 60-64, 65-69, 70-74, 75-80, and 80 or older.


|    Our first steps are to load the applicable libraries. In this file we will be modeling using the caret package and we will also use the tidyverse package, specifically dplyr, to manipulate our data. Our second step will be to load in our raw data and convert the variables we will be using to factors and remove the unused variables. 

```{r Libraries and Raw data, warning=FALSE, message=FALSE,}
library(tidyverse)
library(caret)
library(tree)
```


```{r Read in Data and Set Factors}
rawData <- read.csv("./diabetes_binary_health_indicators_BRFSS2015.csv")

#create a new data frame for cleaning and converts all binary variables into factors
cleaned<- rawData|>
  select(Diabetes_binary,HighBP,HighChol,HvyAlcoholConsump,Smoker,PhysActivity,Age,HeartDiseaseorAttack,MentHlth,BMI)|>
  mutate(Diabetes_binary = factor(Diabetes_binary,labels = c("No","Yes")),
         Age = as.factor(Age),
         HighBP = as.factor(HighBP),
         HighChol = as.factor(HighChol),
         HvyAlcoholConsump = as.factor(HvyAlcoholConsump),
         Smoker = as.factor(Smoker),
         PhysActivity = as.factor(PhysActivity),
         HeartDiseaseorAttack=as.factor(HeartDiseaseorAttack),
         MentHlth=as.factor(MentHlth))
```


### Splitting Data

|   Our next step will be to split the data into a training set and a test set. We will set the seed to 8 to allow for reproduction by others. We then split the data using the \createdatapartition function from the caret package. The split will take 70% of the data and place it into the training set and then the other 30% into the test set. We will use the training set to train our competing models and then use the test set to predict the response variable of the model, diabetes_binary. 


```{r Split Data}
# set seed for predictability
set.seed(8)

# Create a Vector to use to split data. Used createdatapartition to help maintain the ratio of diabates positive to diabetes negative
trainingVec <- createDataPartition(cleaned$Diabetes_binary,
                                   p = .7,
                                   list = FALSE)

# Split data into training and test sets
## Training set
diabetesTrain <- cleaned[trainingVec,]
## Test set
diabetesTest <- cleaned[-trainingVec,]

```


### LogLoss

| For this project we will analyze the logloss of our models rather than looking at the prediction outcome of the model. Logloss is a measurement of prediction used in machine learning. In essence logloss measures how accurate a prediction is to the actual resulting value. The closer the prediction is to the actual value the lower the logloss, lower logloss is preferred. Conversely, the further the prediction is from the actual value the higher the logloss. For example, if a prediction value was 0.95 and the actual was 1 the logloss would be very low, around 0.05, and thus very good. Logloss is computed as such: $$-logLoss = -\frac{1}{N}\sum_{i=1}^{N}\sum_{j=1}^{C}y_{ij}log(p_{ij})$$
This may be a better metric to use in certain cases due to the final logloss. The final logloss is the averaged logloss across the model. This will tell the user how well the model performed overall, with the lower the number the better the model. 
```{r set train control and 5 fold cross-validation using mnLogLoss}
# make Train Control Variable with 5 fold cross-validation
trctrl<- trainControl(method = "cv",
                      number = 5,
                      classProbs = TRUE,
                      summaryFunction = mnLogLoss)
```

# write a paragraph about log models
lower the logloss the better, it is used for classification models in machine learning perfect logloss = 0. logLoss calculates the actual value of an observation and finds the difference between that actual value and the predicted value. The total logloss for a model is the mean logloss for the model. 

# Write a paragraph about logistic regression models and why use

```{r train and fit glm}
# Generalized Linear Model
Diabetes_logFit1<-train(Diabetes_binary ~ HighBP+HighChol+BMI+HvyAlcoholConsump+Smoker,
                       data = diabetesTrain,
                       method = "glm",
                       trControl = trctrl,
                       metric = "logLoss",
                       family = "binomial")

# Generalized Linear Model
Diabetes_logFit2<-train(Diabetes_binary ~ HighBP+HighChol+BMI+HvyAlcoholConsump+Smoker+PhysActivity+Age,
                       data = diabetesTrain,
                       method = "glm",
                       trControl = trctrl,
                       metric = "logLoss",
                       family = "binomial")

# Generalized Linear Model
Diabetes_logFit3<-train(Diabetes_binary ~ .,
                       data = diabetesTrain,
                       method = "glm",
                       trControl = trctrl,
                       metric = "logLoss",
                       family = "binomial",
                       #tuneGrid = expand.grid(nIter = 3)
                       )

# Create a Comparison Table
#Diabetes_logFit3$results<-rename(Diabetes_logFit3$results, parameter = nIter)

Comparison <- data.frame(rbind(Diabetes_logFit1$results,Diabetes_logFit2$results,Diabetes_logFit3$results))|>
  mutate(Model_type = c("1","2","3"))|>
  select(Model_type,logLoss,logLossSD)

Comparison

# Best model = BayesGLM
```

# write paragraph on Classigication trees

```{r Classification trees}
# classTree_Fit<- tree(Diabetes_binary ~                           Age+HighChol+HighBP+HvyAlcoholConsump+Smoker+PhysActivity,
#                      data = diabetesTrain,
#                      split = "deviance"
#                      )
# # summary(classTree_Fit)
# # plot(classTree_Fit)
# classTree_Predict<-predict(classTree_Fit,
#                            newdata = diabetesTest
#                            )

cart_TreeFit<- train(Diabetes_binary ~ HighBP+HighChol+BMI+HvyAlcoholConsump+Smoker,
                       data = diabetesTrain,
                       method = "rpart",
                       trControl = trctrl,
                       metric = "logLoss",
                       #family = "binomial",
                       tuneGrid = expand.grid(cp = seq(0,1,by=.1))
                       )
cart_TreePredict <- predict(cart_TreeFit,
                            newdata = diabetesTest,
                            type = "prob")

cart_TreeFit2<- train(Diabetes_binary ~ HighBP+HighChol+BMI+HvyAlcoholConsump+Smoker+PhysActivity+Age,
                       data = diabetesTrain,
                       method = "rpart",
                       trControl = trctrl,
                       metric = "logLoss",
                       #family = "binomial",
                       tuneGrid = expand.grid(cp = seq(0,1,by=.1))
                       )
cart_TreePredict2 <- predict(cart_TreeFit2,
                            newdata = diabetesTest,
                            type = "prob")

cart_TreeFit3<- train(Diabetes_binary ~ .,
                       data = diabetesTrain,
                       method = "rpart",
                       trControl = trctrl,
                       metric = "logLoss",
                       #family = "binomial",
                       tuneGrid = expand.grid(cp = seq(0,1,by=.1))
                       )
cart_TreePredict3 <- predict(cart_TreeFit3,
                            newdata = diabetesTest,
                            type = "prob")


cart_TreeFit$bestTune
cart_TreeFit2$bestTune
cart_TreeFit3$bestTune

Comparison_tree<- data.frame(rbind(cart_TreeFit$results,
                                   cart_TreeFit2$results,
                                   cart_TreeFit3$results))|>
  mutate(Model_type = c("1","1","1","1","1","1","1","1","1","1","1","2","2","2","2","2","2","2","2","2","2","2","3","3","3","3","3","3","3","3","3","3","3"))|>
  select(Model_type,cp,logLoss,logLossSD)

Comparison_tree

# Best Tree = rpart w/ cp =0.0

```



# random forrest explain random forests

```{r random forests}
rf_Fit<- train(Diabetes_binary ~ HighBP+HighChol+BMI+HvyAlcoholConsump+Smoker,
               data = diabetesTrain,
               method = "ranger",
               trControl = trctrl,
               metric = "logLoss",
               tuneGrid = expand.grid(mtry = 3,
                                      splitrule = "extratrees",
                                      min.node.size = 100)
                       )

rf_Pred<- predict(rf_Fit,
                  newdata = diabetesTest,
                  type = "prob")

rf_Fit2<- train(Diabetes_binary ~ HighBP+HighChol+BMI+HvyAlcoholConsump+Smoker+PhysActivity+Age,
                data = diabetesTrain,
                method = "ranger",
                trControl = trctrl,
                metric = "logLoss",
                tuneGrid = expand.grid(mtry = 3,
                                       splitrule = "extratrees",
                                       min.node.size = 100)
                       )

rf_Pred2<- predict(rf_Fit2,
                  newdata = diabetesTest,
                  type = "prob")

rf_Fit3<- train(Diabetes_binary ~ .,
                data = diabetesTrain,
                method = "ranger",
                trControl = trctrl,
                metric = "logLoss",
                tuneGrid = expand.grid(mtry = 3,
                                       splitrule = "extratrees",
                                       min.node.size = 100)
                       )

rf_Pred3<- predict(rf_Fit3,
                  newdata = diabetesTest,
                  type = "prob")


rf_Fit$results
rf_Fit2$results
rf_Fit3$results

Comparison_forest<- data.frame(
  rbind(
    rf_Fit$results[,c("logLoss","logLossSD")],
    rf_Fit2$results[,c("logLoss","logLossSD")],
    rf_Fit3$results[,c("logLoss","logLossSD")])|>
  mutate(Model_type = c("1","2","3"))|>
  select(Model_type,logLoss,logLossSD))

Comparison_forest

# Best Forest = range
```

# final model selection

```{r compare best models}

bayesGLMPredict<- predict(Diabetes_logFit2,
                          newdata = diabetesTest,
                          type = "prob")
#bayesGLMPredict

cart_TreePredict<-predict(cart_TreeFit,
                          newdata = diabetesTest,
                          type = "prob")

#cart_TreePredict

rf_Pred<-predict(rf_Fit,
                 newdata = diabetesTest,
                 type = "prob")

#rf_Pred

Logloss<- function(real,prediction){
  results<- -1/length(real)*(sum((real*log(prediction)+(1-real)*log(1-prediction))))
  return(results)
}

final_compar<-data.frame(rbind(Logloss(rawData$Diabetes_binary,bayesGLMPredict),
                               Logloss(rawData$Diabetes_binary,cart_TreePredict),
                               Logloss(rawData$Diabetes_binary,rf_Pred)))|>
  mutate(BestModel = c("Bayes GLM", "Classification Tree", "Random Forest"))

names(final_compar)<- c("LogLoss","Mehod")
final_compar

# best model = Classification tree with all variables
```

