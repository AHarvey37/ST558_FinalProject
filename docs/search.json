[
  {
    "objectID": "Modeling.html",
    "href": "Modeling.html",
    "title": "Modeling",
    "section": "",
    "text": "This project will use the “diabetes_binary_health_indicators_BRFSS2015” data set. According Kaggle.com this data set contains 253,680 observation where the response variable is whether the observed individual does or does not not have diabetes (0 or 1 respectively) or has pre-diabetes/diabetes (1). The original data set contains 22 variables most of which are categorical. We have limited the variables to those below:\n\nDiabetes_binary (response variable) - This variable illustrates if the subject has or does not have diabetes.\nHighBP (Categorical) - This is a binary variable describing whether or not the subject has high blood pressure.\nHighChol (Categorical) - This is a binary variable describing whether or not the subject has high cholesterol.\nSmoker (Categorical) - This is a binary variable describing whether or not the subject has smoked at least 100 cigarettes in their lifetime. 100 cigarettes is equal to 5 packs.\nHeartDiseaseorAttack (Categorical) - This is a binary variable describing whether or not the subject has coronary heart disease or myocardial infarction (a heart attack).\n\nPhysActivity (Categorical) - This is a binary variable describing whether or not the subject has conducted physical activity (outside of their job) in the past 30 days.\nHvyAlcoholConsump (Categorical) - This is a binary variable describing whether or not the subject, if male, consumes 14 or more alcoholic drinks per week, or if female, consumes 7 or more alcoholic drinks per week.\nMentHlth - This variable is a scale from 1 to 30 of how many days the subject has had POOR mental health.\nAge (Categorical) - This variable breaks age into 13 levels which are 18-24, 25-29, 30-34, 35-39, 40-44, 45-49, 50-54, 55-59, 60-64, 65-69, 70-74, 75-80, and 80 or older.\n\n   Our first steps are to load the applicable libraries. In this file we will be modeling using the caret package and we will also use the tidyverse package, specifically dplyr, to manipulate our data. Our second step will be to load in our raw data and convert the variables we will be using to factors and remove the unused variables.\n\nlibrary(tidyverse)\nlibrary(caret)\nlibrary(tree)\n\n\nrawData &lt;- read.csv(\"./diabetes_binary_health_indicators_BRFSS2015.csv\")\n\n#create a new data frame for cleaning and converts all binary variables into factors\ncleaned&lt;- rawData|&gt;\n  select(Diabetes_binary,HighBP,HighChol,HvyAlcoholConsump,Smoker,PhysActivity,Age,HeartDiseaseorAttack,MentHlth,BMI)|&gt;\n  drop_na()|&gt;\n  mutate(Diabetes_binary = factor(Diabetes_binary,labels = c(\"No\",\"Yes\")),\n         Age = as.factor(Age),\n         HighBP = as.factor(HighBP),\n         HighChol = as.factor(HighChol),\n         HvyAlcoholConsump = as.factor(HvyAlcoholConsump),\n         Smoker = as.factor(Smoker),\n         PhysActivity = as.factor(PhysActivity),\n         HeartDiseaseorAttack=as.factor(HeartDiseaseorAttack),\n         MentHlth=as.factor(MentHlth))\n\n\n\n  Our next step will be to split the data into a training set and a test set. We will set the seed to 8 to allow for reproduction by others. We then split the data using the function from the caret package. The split will take 70% of the data and place it into the training set and then the other 30% into the test set. We will use the training set to train our competing models and then use the test set to predict the response variable of the model, diabetes_binary.\n\n# set seed for predictability\nset.seed(8)\n\n# Create a Vector to use to split data. Used createdatapartition to help maintain the ratio of diabates positive to diabetes negative\ntrainingVec &lt;- createDataPartition(cleaned$Diabetes_binary,\n                                   p = .7,\n                                   list = FALSE)\n\n# Split data into training and test sets\n## Training set\ndiabetesTrain &lt;- cleaned[trainingVec,]\n## Test set\ndiabetesTest &lt;- cleaned[-trainingVec,]\n\n\n\n\nFor this project we will analyze the logloss of our models rather than looking at the prediction outcome of the model. Logloss is a measurement of prediction used in machine learning. In essence logloss measures how accurate a prediction is to the actual resulting value. The closer the prediction is to the actual value the lower the logloss, lower logloss is preferred. Conversely, the further the prediction is from the actual value the higher the logloss. For example, if a prediction value was 0.95 and the actual was 1 the logloss would be very low, around 0.05, and thus very good. Logloss is computed as such: \\[-logLoss = -\\frac{1}{N}\\sum_{i=1}^{N}\\sum_{j=1}^{C}y_{ij}log(p_{ij})\\]\nThis may be a better metric to use in certain cases due to the final logloss. The final logloss is the averaged logloss across the model. This will tell the user how well the model performed overall, with the lower the number the better the model.\n\n# make Train Control Variable with 5 fold cross-validation\ntrctrl&lt;- trainControl(method = \"cv\",\n                      number = 5,\n                      classProbs = TRUE,\n                      summaryFunction = mnLogLoss)\n\n\n\n\n\n   For our first model type we will use a Bayes Logistic regression model. Logistic regression models explores the relationship between the independent variables and the response variable. In this case we are looking at a binary classification, whether the response variable will be 0 or 1 (has diabetes or does not), so the model will use the logit function to predict this relationship.\n\n# Generalized Linear Model\nDiabetes_logFit1&lt;-train(Diabetes_binary ~ HighBP+HighChol+BMI+HvyAlcoholConsump+Smoker,\n                       data = diabetesTrain,\n                       method = \"bayesglm\",\n                       trControl = trctrl,\n                       metric = \"logLoss\",\n                       family = \"binomial\")\n\n# Generalized Linear Model\nDiabetes_logFit2&lt;-train(Diabetes_binary ~ HighBP+HighChol+BMI+HvyAlcoholConsump+Smoker+PhysActivity+Age,\n                       data = diabetesTrain,\n                       method = \"bayesglm\",\n                       trControl = trctrl,\n                       metric = \"logLoss\",\n                       family = \"binomial\")\n\n# Generalized Linear Model\nDiabetes_logFit3&lt;-train(Diabetes_binary ~ .,\n                       data = diabetesTrain,\n                       method = \"bayesglm\",\n                       trControl = trctrl,\n                       metric = \"logLoss\",\n                       family = \"binomial\",\n                       #tuneGrid = expand.grid(nIter = 3)\n                       )\n\n# Create a Comparison Table\n#Diabetes_logFit3$results&lt;-rename(Diabetes_logFit3$results, parameter = nIter)\n\nComparison &lt;- data.frame(rbind(Diabetes_logFit1$results,Diabetes_logFit2$results,Diabetes_logFit3$results))|&gt;\n  mutate(Model_type = c(\"1\",\"2\",\"3\"))|&gt;\n  select(Model_type,logLoss,logLossSD)\n\nComparison\n\n  Model_type   logLoss    logLossSD\n1          1 0.3465360 0.0007585621\n2          2 0.3376574 0.0011227018\n3          3 0.3343855 0.0009810140\n\n# Best model = BayesGLM\n\n\n\n\n  The second model type that will be used is the classification tree method. This model takes is a decision tree and we will use a ‘CART’ method. The classification and regression tree (CART) looks at how the response variable interacts with all other variables by growing a tree. A tree is grown by splitting each prediction into its own path based on a specific prediction value, i.e. if a prediction value is above a certain value then it will fork to one direction and if it is under the specified value then it will fork to the other direction. This will continue until each node reaches a terminal level. This model type is prone to over-fitting and thus we must implement pruning methods to ensure that the model is not over fitted.\n\ncart_TreeFit&lt;- train(Diabetes_binary ~ HighBP+HighChol+BMI+HvyAlcoholConsump+Smoker,\n                       data = diabetesTrain,\n                       method = \"rpart\",\n                       trControl = trctrl,\n                       metric = \"logLoss\",\n                       #family = \"binomial\",\n                       tuneGrid = expand.grid(cp = seq(0,1,by=.1))\n                       )\ncart_TreePredict &lt;- predict(cart_TreeFit,\n                            newdata = diabetesTest,\n                            type = \"prob\")\nhead(cart_TreePredict)\n\n          No        Yes\n2  0.9600715 0.03992853\n3  0.8123233 0.18767666\n6  0.8123233 0.18767666\n13 0.9600715 0.03992853\n20 0.9600715 0.03992853\n23 0.9600715 0.03992853\n\ncart_TreeFit2&lt;- train(Diabetes_binary ~ HighBP+HighChol+BMI+HvyAlcoholConsump+Smoker+PhysActivity+Age,\n                       data = diabetesTrain,\n                       method = \"rpart\",\n                       trControl = trctrl,\n                       metric = \"logLoss\",\n                       #family = \"binomial\",\n                       tuneGrid = expand.grid(cp = seq(0,1,by=.1))\n                       )\ncart_TreePredict2 &lt;- predict(cart_TreeFit2,\n                            newdata = diabetesTest,\n                            type = \"prob\")\nhead(cart_TreePredict2)\n\n          No        Yes\n2  0.9600715 0.03992853\n3  0.6879451 0.31205487\n6  0.8182285 0.18177148\n13 0.9600715 0.03992853\n20 0.9600715 0.03992853\n23 0.9600715 0.03992853\n\ncart_TreeFit3&lt;- train(Diabetes_binary ~ .,\n                       data = diabetesTrain,\n                       method = \"rpart\",\n                       trControl = trctrl,\n                       metric = \"logLoss\",\n                       #family = \"binomial\",\n                       tuneGrid = expand.grid(cp = seq(0,1,by=.1))\n                       )\ncart_TreePredict3 &lt;- predict(cart_TreeFit3,\n                            newdata = diabetesTest,\n                            type = \"prob\")\nhead(cart_TreePredict3)\n\n          No        Yes\n2  0.9712655 0.02873455\n3  0.6961039 0.30389610\n6  0.8132939 0.18670611\n13 0.9712655 0.02873455\n20 0.9168060 0.08319398\n23 0.9712655 0.02873455\n\ncart_TreeFit$results\n\n    cp   logLoss    logLossSD\n1  0.0 0.3525920 1.932811e-03\n2  0.1 0.4037576 2.563868e-05\n3  0.2 0.4037576 2.563868e-05\n4  0.3 0.4037576 2.563868e-05\n5  0.4 0.4037576 2.563868e-05\n6  0.5 0.4037576 2.563868e-05\n7  0.6 0.4037576 2.563868e-05\n8  0.7 0.4037576 2.563868e-05\n9  0.8 0.4037576 2.563868e-05\n10 0.9 0.4037576 2.563868e-05\n11 1.0 0.4037576 2.563868e-05\n\ncart_TreeFit2$results\n\n    cp   logLoss    logLossSD\n1  0.0 0.3528396 3.848571e-03\n2  0.1 0.4037576 2.563868e-05\n3  0.2 0.4037576 2.563868e-05\n4  0.3 0.4037576 2.563868e-05\n5  0.4 0.4037576 2.563868e-05\n6  0.5 0.4037576 2.563868e-05\n7  0.6 0.4037576 2.563868e-05\n8  0.7 0.4037576 2.563868e-05\n9  0.8 0.4037576 2.563868e-05\n10 0.9 0.4037576 2.563868e-05\n11 1.0 0.4037576 2.563868e-05\n\ncart_TreeFit3$results\n\n    cp   logLoss    logLossSD\n1  0.0 0.3563362 4.340225e-03\n2  0.1 0.4037576 2.563868e-05\n3  0.2 0.4037576 2.563868e-05\n4  0.3 0.4037576 2.563868e-05\n5  0.4 0.4037576 2.563868e-05\n6  0.5 0.4037576 2.563868e-05\n7  0.6 0.4037576 2.563868e-05\n8  0.7 0.4037576 2.563868e-05\n9  0.8 0.4037576 2.563868e-05\n10 0.9 0.4037576 2.563868e-05\n11 1.0 0.4037576 2.563868e-05\n\nComparison_tree&lt;- data.frame(rbind(cart_TreeFit$results[1:3,],\n                                   cart_TreeFit2$results[1:3,],\n                                   cart_TreeFit3$results[1:3,]))|&gt;\n   mutate(Model_type = c(\"1\",\"1\",\"1\",\"2\",\"2\",\"2\",\"3\",\"3\",\"3\"))|&gt;\n  select(Model_type,cp,logLoss,logLossSD)\n\nComparison_tree\n\n  Model_type  cp   logLoss    logLossSD\n1          1 0.0 0.3525920 1.932811e-03\n2          1 0.1 0.4037576 2.563868e-05\n3          1 0.2 0.4037576 2.563868e-05\n4          2 0.0 0.3528396 3.848571e-03\n5          2 0.1 0.4037576 2.563868e-05\n6          2 0.2 0.4037576 2.563868e-05\n7          3 0.0 0.3563362 4.340225e-03\n8          3 0.1 0.4037576 2.563868e-05\n9          3 0.2 0.4037576 2.563868e-05\n\n# Best Tree = rpart w/ cp =0.0\n\n\n\n\n  The final model type that we will examine is the Random Forest. The Random Forest method is very similar to the classification tree model type but instead of growing one tree model type grows many tree. While this may seem like it would be easily more beneficial to the classification tree method the random forest loses some of the specificity that the classification tree has since it averages many trees together. This average is an attempt to increase accuracy but it does not always improve.\n\nrf_Fit&lt;- train(Diabetes_binary ~ HighBP+HighChol+BMI+HvyAlcoholConsump+Smoker,\n               data = diabetesTrain,\n               method = \"ranger\",\n               trControl = trctrl,\n               metric = \"logLoss\",\n               tuneGrid = expand.grid(mtry = 3,\n                                      splitrule = \"extratrees\",\n                                      min.node.size = 100)\n                       )\n\nrf_Pred&lt;- predict(rf_Fit,\n                  newdata = diabetesTest,\n                  type = \"prob\")\n\nrf_Fit2&lt;- train(Diabetes_binary ~ HighBP+HighChol+BMI+HvyAlcoholConsump+Smoker+PhysActivity+Age,\n                data = diabetesTrain,\n                method = \"ranger\",\n                trControl = trctrl,\n                metric = \"logLoss\",\n                tuneGrid = expand.grid(mtry = 3,\n                                       splitrule = \"extratrees\",\n                                       min.node.size = 100)\n                       )\n\nrf_Pred2&lt;- predict(rf_Fit2,\n                  newdata = diabetesTest,\n                  type = \"prob\")\n\nrf_Fit3&lt;- train(Diabetes_binary ~ .,\n                data = diabetesTrain,\n                method = \"ranger\",\n                trControl = trctrl,\n                metric = \"logLoss\",\n                tuneGrid = expand.grid(mtry = 3,\n                                       splitrule = \"extratrees\",\n                                       min.node.size = 100)\n                       )\n\nrf_Pred3&lt;- predict(rf_Fit3,\n                  newdata = diabetesTest,\n                  type = \"prob\")\n\n\nrf_Fit$results\n\n  mtry  splitrule min.node.size   logLoss    logLossSD\n1    3 extratrees           100 0.3470174 0.0009277561\n\nrf_Fit2$results\n\n  mtry  splitrule min.node.size   logLoss    logLossSD\n1    3 extratrees           100 0.3442571 0.0009484639\n\nrf_Fit3$results\n\n  mtry  splitrule min.node.size   logLoss   logLossSD\n1    3 extratrees           100 0.3452643 0.001033621\n\nComparison_forest&lt;- data.frame(\n  rbind(\n    rf_Fit$results[,c(\"logLoss\",\"logLossSD\")],\n    rf_Fit2$results[,c(\"logLoss\",\"logLossSD\")],\n    rf_Fit3$results[,c(\"logLoss\",\"logLossSD\")])|&gt;\n  mutate(Model_type = c(\"1\",\"2\",\"3\"))|&gt;\n  select(Model_type,logLoss,logLossSD))\n\nComparison_forest\n\n  Model_type   logLoss    logLossSD\n1          1 0.3470174 0.0009277561\n2          2 0.3442571 0.0009484639\n3          3 0.3452643 0.0010336206\n\n# Best Forest = range\n\n\n\n\n  In this final section we take the best models from above and compare them to see which is the overall best model. We do this be comparing the lowest logloss from the three models when they are predicting on the test data set. Since we have already trained all three models we will use the predict function to predict the results of the test set. We will then look at the logloss of each model and select the lowest logloss as our best model. After comparing our models we find that Random Forest is our best model.\n\nbayesGLMPredict&lt;- predict(Diabetes_logFit2,\n                          newdata = diabetesTest,\n                          type = \"prob\")|&gt;\n  as_tibble()\nbayesGLMPredict\n\n# A tibble: 76,103 × 2\n      No    Yes\n   &lt;dbl&gt;  &lt;dbl&gt;\n 1 0.961 0.0387\n 2 0.697 0.303 \n 3 0.745 0.255 \n 4 0.942 0.0579\n 5 0.977 0.0227\n 6 0.957 0.0433\n 7 0.472 0.528 \n 8 0.775 0.225 \n 9 0.980 0.0203\n10 0.934 0.0660\n# ℹ 76,093 more rows\n\ncart_TreePredict&lt;-predict(cart_TreeFit,\n                          newdata = diabetesTest,\n                          type = \"prob\")|&gt;\n  as_tibble()\n\ncart_TreePredict\n\n# A tibble: 76,103 × 2\n      No    Yes\n   &lt;dbl&gt;  &lt;dbl&gt;\n 1 0.960 0.0399\n 2 0.812 0.188 \n 3 0.812 0.188 \n 4 0.960 0.0399\n 5 0.960 0.0399\n 6 0.960 0.0399\n 7 0.610 0.390 \n 8 0.776 0.224 \n 9 0.960 0.0399\n10 0.960 0.0399\n# ℹ 76,093 more rows\n\nrf_Pred&lt;-predict(rf_Fit2,\n                 newdata = diabetesTest,\n                 type = \"prob\")|&gt;\n  as_tibble()\n\nrf_Pred\n\n# A tibble: 76,103 × 2\n      No    Yes\n   &lt;dbl&gt;  &lt;dbl&gt;\n 1 0.956 0.0440\n 2 0.667 0.333 \n 3 0.736 0.264 \n 4 0.936 0.0643\n 5 0.935 0.0650\n 6 0.944 0.0561\n 7 0.616 0.384 \n 8 0.824 0.176 \n 9 0.969 0.0310\n10 0.934 0.0661\n# ℹ 76,093 more rows\n\nLogloss&lt;- function(real,prediction){\n  results&lt;- -1/length(real)*sum(real*log10(prediction)+(1-real)*log10(1-prediction))\n  return(results)\n}\n\ntest&lt;- function(real,pred){\n  results&lt;-sum(log10(pred))\n  return(results)\n}\n\n\ntest(rawData$Diabetes_binary,cart_TreePredict)\n\n[1] -Inf\n\nfinal_compar&lt;-data.frame(rbind(Logloss(rawData$Diabetes_binary,bayesGLMPredict),\n                               Logloss(rawData$Diabetes_binary,cart_TreePredict),\n                               Logloss(rawData$Diabetes_binary,rf_Pred)))|&gt;\n  mutate(BestModel = c(\"Bayes GLM\", \"Classification Tree\", \"Random Forest\"))\n\nnames(final_compar)&lt;- c(\"LogLoss\",\"Mehod\")\nfinal_compar\n\n    LogLoss               Mehod\n1 0.3431751           Bayes GLM\n2       NaN Classification Tree\n3 0.3174830       Random Forest\n\n# best model = Random Forest with 7 variables"
  },
  {
    "objectID": "Modeling.html#introduction-section",
    "href": "Modeling.html#introduction-section",
    "title": "Modeling",
    "section": "",
    "text": "This project will use the “diabetes_binary_health_indicators_BRFSS2015” data set. According Kaggle.com this data set contains 253,680 observation where the response variable is whether the observed individual does or does not not have diabetes (0 or 1 respectively) or has pre-diabetes/diabetes (1). The original data set contains 22 variables most of which are categorical. We have limited the variables to those below:\n\nDiabetes_binary (response variable) - This variable illustrates if the subject has or does not have diabetes.\nHighBP (Categorical) - This is a binary variable describing whether or not the subject has high blood pressure.\nHighChol (Categorical) - This is a binary variable describing whether or not the subject has high cholesterol.\nSmoker (Categorical) - This is a binary variable describing whether or not the subject has smoked at least 100 cigarettes in their lifetime. 100 cigarettes is equal to 5 packs.\nHeartDiseaseorAttack (Categorical) - This is a binary variable describing whether or not the subject has coronary heart disease or myocardial infarction (a heart attack).\n\nPhysActivity (Categorical) - This is a binary variable describing whether or not the subject has conducted physical activity (outside of their job) in the past 30 days.\nHvyAlcoholConsump (Categorical) - This is a binary variable describing whether or not the subject, if male, consumes 14 or more alcoholic drinks per week, or if female, consumes 7 or more alcoholic drinks per week.\nMentHlth - This variable is a scale from 1 to 30 of how many days the subject has had POOR mental health.\nAge (Categorical) - This variable breaks age into 13 levels which are 18-24, 25-29, 30-34, 35-39, 40-44, 45-49, 50-54, 55-59, 60-64, 65-69, 70-74, 75-80, and 80 or older.\n\n   Our first steps are to load the applicable libraries. In this file we will be modeling using the caret package and we will also use the tidyverse package, specifically dplyr, to manipulate our data. Our second step will be to load in our raw data and convert the variables we will be using to factors and remove the unused variables.\n\nlibrary(tidyverse)\nlibrary(caret)\nlibrary(tree)\n\n\nrawData &lt;- read.csv(\"./diabetes_binary_health_indicators_BRFSS2015.csv\")\n\n#create a new data frame for cleaning and converts all binary variables into factors\ncleaned&lt;- rawData|&gt;\n  select(Diabetes_binary,HighBP,HighChol,HvyAlcoholConsump,Smoker,PhysActivity,Age,HeartDiseaseorAttack,MentHlth,BMI)|&gt;\n  drop_na()|&gt;\n  mutate(Diabetes_binary = factor(Diabetes_binary,labels = c(\"No\",\"Yes\")),\n         Age = as.factor(Age),\n         HighBP = as.factor(HighBP),\n         HighChol = as.factor(HighChol),\n         HvyAlcoholConsump = as.factor(HvyAlcoholConsump),\n         Smoker = as.factor(Smoker),\n         PhysActivity = as.factor(PhysActivity),\n         HeartDiseaseorAttack=as.factor(HeartDiseaseorAttack),\n         MentHlth=as.factor(MentHlth))\n\n\n\n  Our next step will be to split the data into a training set and a test set. We will set the seed to 8 to allow for reproduction by others. We then split the data using the function from the caret package. The split will take 70% of the data and place it into the training set and then the other 30% into the test set. We will use the training set to train our competing models and then use the test set to predict the response variable of the model, diabetes_binary.\n\n# set seed for predictability\nset.seed(8)\n\n# Create a Vector to use to split data. Used createdatapartition to help maintain the ratio of diabates positive to diabetes negative\ntrainingVec &lt;- createDataPartition(cleaned$Diabetes_binary,\n                                   p = .7,\n                                   list = FALSE)\n\n# Split data into training and test sets\n## Training set\ndiabetesTrain &lt;- cleaned[trainingVec,]\n## Test set\ndiabetesTest &lt;- cleaned[-trainingVec,]\n\n\n\n\nFor this project we will analyze the logloss of our models rather than looking at the prediction outcome of the model. Logloss is a measurement of prediction used in machine learning. In essence logloss measures how accurate a prediction is to the actual resulting value. The closer the prediction is to the actual value the lower the logloss, lower logloss is preferred. Conversely, the further the prediction is from the actual value the higher the logloss. For example, if a prediction value was 0.95 and the actual was 1 the logloss would be very low, around 0.05, and thus very good. Logloss is computed as such: \\[-logLoss = -\\frac{1}{N}\\sum_{i=1}^{N}\\sum_{j=1}^{C}y_{ij}log(p_{ij})\\]\nThis may be a better metric to use in certain cases due to the final logloss. The final logloss is the averaged logloss across the model. This will tell the user how well the model performed overall, with the lower the number the better the model.\n\n# make Train Control Variable with 5 fold cross-validation\ntrctrl&lt;- trainControl(method = \"cv\",\n                      number = 5,\n                      classProbs = TRUE,\n                      summaryFunction = mnLogLoss)"
  },
  {
    "objectID": "Modeling.html#logistic-regression-models",
    "href": "Modeling.html#logistic-regression-models",
    "title": "Modeling",
    "section": "",
    "text": "For our first model type we will use a Bayes Logistic regression model. Logistic regression models explores the relationship between the independent variables and the response variable. In this case we are looking at a binary classification, whether the response variable will be 0 or 1 (has diabetes or does not), so the model will use the logit function to predict this relationship.\n\n# Generalized Linear Model\nDiabetes_logFit1&lt;-train(Diabetes_binary ~ HighBP+HighChol+BMI+HvyAlcoholConsump+Smoker,\n                       data = diabetesTrain,\n                       method = \"bayesglm\",\n                       trControl = trctrl,\n                       metric = \"logLoss\",\n                       family = \"binomial\")\n\n# Generalized Linear Model\nDiabetes_logFit2&lt;-train(Diabetes_binary ~ HighBP+HighChol+BMI+HvyAlcoholConsump+Smoker+PhysActivity+Age,\n                       data = diabetesTrain,\n                       method = \"bayesglm\",\n                       trControl = trctrl,\n                       metric = \"logLoss\",\n                       family = \"binomial\")\n\n# Generalized Linear Model\nDiabetes_logFit3&lt;-train(Diabetes_binary ~ .,\n                       data = diabetesTrain,\n                       method = \"bayesglm\",\n                       trControl = trctrl,\n                       metric = \"logLoss\",\n                       family = \"binomial\",\n                       #tuneGrid = expand.grid(nIter = 3)\n                       )\n\n# Create a Comparison Table\n#Diabetes_logFit3$results&lt;-rename(Diabetes_logFit3$results, parameter = nIter)\n\nComparison &lt;- data.frame(rbind(Diabetes_logFit1$results,Diabetes_logFit2$results,Diabetes_logFit3$results))|&gt;\n  mutate(Model_type = c(\"1\",\"2\",\"3\"))|&gt;\n  select(Model_type,logLoss,logLossSD)\n\nComparison\n\n  Model_type   logLoss    logLossSD\n1          1 0.3465360 0.0007585621\n2          2 0.3376574 0.0011227018\n3          3 0.3343855 0.0009810140\n\n# Best model = BayesGLM"
  },
  {
    "objectID": "Modeling.html#classification-trees",
    "href": "Modeling.html#classification-trees",
    "title": "Modeling",
    "section": "",
    "text": "The second model type that will be used is the classification tree method. This model takes is a decision tree and we will use a ‘CART’ method. The classification and regression tree (CART) looks at how the response variable interacts with all other variables by growing a tree. A tree is grown by splitting each prediction into its own path based on a specific prediction value, i.e. if a prediction value is above a certain value then it will fork to one direction and if it is under the specified value then it will fork to the other direction. This will continue until each node reaches a terminal level. This model type is prone to over-fitting and thus we must implement pruning methods to ensure that the model is not over fitted.\n\ncart_TreeFit&lt;- train(Diabetes_binary ~ HighBP+HighChol+BMI+HvyAlcoholConsump+Smoker,\n                       data = diabetesTrain,\n                       method = \"rpart\",\n                       trControl = trctrl,\n                       metric = \"logLoss\",\n                       #family = \"binomial\",\n                       tuneGrid = expand.grid(cp = seq(0,1,by=.1))\n                       )\ncart_TreePredict &lt;- predict(cart_TreeFit,\n                            newdata = diabetesTest,\n                            type = \"prob\")\nhead(cart_TreePredict)\n\n          No        Yes\n2  0.9600715 0.03992853\n3  0.8123233 0.18767666\n6  0.8123233 0.18767666\n13 0.9600715 0.03992853\n20 0.9600715 0.03992853\n23 0.9600715 0.03992853\n\ncart_TreeFit2&lt;- train(Diabetes_binary ~ HighBP+HighChol+BMI+HvyAlcoholConsump+Smoker+PhysActivity+Age,\n                       data = diabetesTrain,\n                       method = \"rpart\",\n                       trControl = trctrl,\n                       metric = \"logLoss\",\n                       #family = \"binomial\",\n                       tuneGrid = expand.grid(cp = seq(0,1,by=.1))\n                       )\ncart_TreePredict2 &lt;- predict(cart_TreeFit2,\n                            newdata = diabetesTest,\n                            type = \"prob\")\nhead(cart_TreePredict2)\n\n          No        Yes\n2  0.9600715 0.03992853\n3  0.6879451 0.31205487\n6  0.8182285 0.18177148\n13 0.9600715 0.03992853\n20 0.9600715 0.03992853\n23 0.9600715 0.03992853\n\ncart_TreeFit3&lt;- train(Diabetes_binary ~ .,\n                       data = diabetesTrain,\n                       method = \"rpart\",\n                       trControl = trctrl,\n                       metric = \"logLoss\",\n                       #family = \"binomial\",\n                       tuneGrid = expand.grid(cp = seq(0,1,by=.1))\n                       )\ncart_TreePredict3 &lt;- predict(cart_TreeFit3,\n                            newdata = diabetesTest,\n                            type = \"prob\")\nhead(cart_TreePredict3)\n\n          No        Yes\n2  0.9712655 0.02873455\n3  0.6961039 0.30389610\n6  0.8132939 0.18670611\n13 0.9712655 0.02873455\n20 0.9168060 0.08319398\n23 0.9712655 0.02873455\n\ncart_TreeFit$results\n\n    cp   logLoss    logLossSD\n1  0.0 0.3525920 1.932811e-03\n2  0.1 0.4037576 2.563868e-05\n3  0.2 0.4037576 2.563868e-05\n4  0.3 0.4037576 2.563868e-05\n5  0.4 0.4037576 2.563868e-05\n6  0.5 0.4037576 2.563868e-05\n7  0.6 0.4037576 2.563868e-05\n8  0.7 0.4037576 2.563868e-05\n9  0.8 0.4037576 2.563868e-05\n10 0.9 0.4037576 2.563868e-05\n11 1.0 0.4037576 2.563868e-05\n\ncart_TreeFit2$results\n\n    cp   logLoss    logLossSD\n1  0.0 0.3528396 3.848571e-03\n2  0.1 0.4037576 2.563868e-05\n3  0.2 0.4037576 2.563868e-05\n4  0.3 0.4037576 2.563868e-05\n5  0.4 0.4037576 2.563868e-05\n6  0.5 0.4037576 2.563868e-05\n7  0.6 0.4037576 2.563868e-05\n8  0.7 0.4037576 2.563868e-05\n9  0.8 0.4037576 2.563868e-05\n10 0.9 0.4037576 2.563868e-05\n11 1.0 0.4037576 2.563868e-05\n\ncart_TreeFit3$results\n\n    cp   logLoss    logLossSD\n1  0.0 0.3563362 4.340225e-03\n2  0.1 0.4037576 2.563868e-05\n3  0.2 0.4037576 2.563868e-05\n4  0.3 0.4037576 2.563868e-05\n5  0.4 0.4037576 2.563868e-05\n6  0.5 0.4037576 2.563868e-05\n7  0.6 0.4037576 2.563868e-05\n8  0.7 0.4037576 2.563868e-05\n9  0.8 0.4037576 2.563868e-05\n10 0.9 0.4037576 2.563868e-05\n11 1.0 0.4037576 2.563868e-05\n\nComparison_tree&lt;- data.frame(rbind(cart_TreeFit$results[1:3,],\n                                   cart_TreeFit2$results[1:3,],\n                                   cart_TreeFit3$results[1:3,]))|&gt;\n   mutate(Model_type = c(\"1\",\"1\",\"1\",\"2\",\"2\",\"2\",\"3\",\"3\",\"3\"))|&gt;\n  select(Model_type,cp,logLoss,logLossSD)\n\nComparison_tree\n\n  Model_type  cp   logLoss    logLossSD\n1          1 0.0 0.3525920 1.932811e-03\n2          1 0.1 0.4037576 2.563868e-05\n3          1 0.2 0.4037576 2.563868e-05\n4          2 0.0 0.3528396 3.848571e-03\n5          2 0.1 0.4037576 2.563868e-05\n6          2 0.2 0.4037576 2.563868e-05\n7          3 0.0 0.3563362 4.340225e-03\n8          3 0.1 0.4037576 2.563868e-05\n9          3 0.2 0.4037576 2.563868e-05\n\n# Best Tree = rpart w/ cp =0.0"
  },
  {
    "objectID": "Modeling.html#random-forests",
    "href": "Modeling.html#random-forests",
    "title": "Modeling",
    "section": "",
    "text": "The final model type that we will examine is the Random Forest. The Random Forest method is very similar to the classification tree model type but instead of growing one tree model type grows many tree. While this may seem like it would be easily more beneficial to the classification tree method the random forest loses some of the specificity that the classification tree has since it averages many trees together. This average is an attempt to increase accuracy but it does not always improve.\n\nrf_Fit&lt;- train(Diabetes_binary ~ HighBP+HighChol+BMI+HvyAlcoholConsump+Smoker,\n               data = diabetesTrain,\n               method = \"ranger\",\n               trControl = trctrl,\n               metric = \"logLoss\",\n               tuneGrid = expand.grid(mtry = 3,\n                                      splitrule = \"extratrees\",\n                                      min.node.size = 100)\n                       )\n\nrf_Pred&lt;- predict(rf_Fit,\n                  newdata = diabetesTest,\n                  type = \"prob\")\n\nrf_Fit2&lt;- train(Diabetes_binary ~ HighBP+HighChol+BMI+HvyAlcoholConsump+Smoker+PhysActivity+Age,\n                data = diabetesTrain,\n                method = \"ranger\",\n                trControl = trctrl,\n                metric = \"logLoss\",\n                tuneGrid = expand.grid(mtry = 3,\n                                       splitrule = \"extratrees\",\n                                       min.node.size = 100)\n                       )\n\nrf_Pred2&lt;- predict(rf_Fit2,\n                  newdata = diabetesTest,\n                  type = \"prob\")\n\nrf_Fit3&lt;- train(Diabetes_binary ~ .,\n                data = diabetesTrain,\n                method = \"ranger\",\n                trControl = trctrl,\n                metric = \"logLoss\",\n                tuneGrid = expand.grid(mtry = 3,\n                                       splitrule = \"extratrees\",\n                                       min.node.size = 100)\n                       )\n\nrf_Pred3&lt;- predict(rf_Fit3,\n                  newdata = diabetesTest,\n                  type = \"prob\")\n\n\nrf_Fit$results\n\n  mtry  splitrule min.node.size   logLoss    logLossSD\n1    3 extratrees           100 0.3470174 0.0009277561\n\nrf_Fit2$results\n\n  mtry  splitrule min.node.size   logLoss    logLossSD\n1    3 extratrees           100 0.3442571 0.0009484639\n\nrf_Fit3$results\n\n  mtry  splitrule min.node.size   logLoss   logLossSD\n1    3 extratrees           100 0.3452643 0.001033621\n\nComparison_forest&lt;- data.frame(\n  rbind(\n    rf_Fit$results[,c(\"logLoss\",\"logLossSD\")],\n    rf_Fit2$results[,c(\"logLoss\",\"logLossSD\")],\n    rf_Fit3$results[,c(\"logLoss\",\"logLossSD\")])|&gt;\n  mutate(Model_type = c(\"1\",\"2\",\"3\"))|&gt;\n  select(Model_type,logLoss,logLossSD))\n\nComparison_forest\n\n  Model_type   logLoss    logLossSD\n1          1 0.3470174 0.0009277561\n2          2 0.3442571 0.0009484639\n3          3 0.3452643 0.0010336206\n\n# Best Forest = range"
  },
  {
    "objectID": "Modeling.html#final-model-selection",
    "href": "Modeling.html#final-model-selection",
    "title": "Modeling",
    "section": "",
    "text": "In this final section we take the best models from above and compare them to see which is the overall best model. We do this be comparing the lowest logloss from the three models when they are predicting on the test data set. Since we have already trained all three models we will use the predict function to predict the results of the test set. We will then look at the logloss of each model and select the lowest logloss as our best model. After comparing our models we find that Random Forest is our best model.\n\nbayesGLMPredict&lt;- predict(Diabetes_logFit2,\n                          newdata = diabetesTest,\n                          type = \"prob\")|&gt;\n  as_tibble()\nbayesGLMPredict\n\n# A tibble: 76,103 × 2\n      No    Yes\n   &lt;dbl&gt;  &lt;dbl&gt;\n 1 0.961 0.0387\n 2 0.697 0.303 \n 3 0.745 0.255 \n 4 0.942 0.0579\n 5 0.977 0.0227\n 6 0.957 0.0433\n 7 0.472 0.528 \n 8 0.775 0.225 \n 9 0.980 0.0203\n10 0.934 0.0660\n# ℹ 76,093 more rows\n\ncart_TreePredict&lt;-predict(cart_TreeFit,\n                          newdata = diabetesTest,\n                          type = \"prob\")|&gt;\n  as_tibble()\n\ncart_TreePredict\n\n# A tibble: 76,103 × 2\n      No    Yes\n   &lt;dbl&gt;  &lt;dbl&gt;\n 1 0.960 0.0399\n 2 0.812 0.188 \n 3 0.812 0.188 \n 4 0.960 0.0399\n 5 0.960 0.0399\n 6 0.960 0.0399\n 7 0.610 0.390 \n 8 0.776 0.224 \n 9 0.960 0.0399\n10 0.960 0.0399\n# ℹ 76,093 more rows\n\nrf_Pred&lt;-predict(rf_Fit2,\n                 newdata = diabetesTest,\n                 type = \"prob\")|&gt;\n  as_tibble()\n\nrf_Pred\n\n# A tibble: 76,103 × 2\n      No    Yes\n   &lt;dbl&gt;  &lt;dbl&gt;\n 1 0.956 0.0440\n 2 0.667 0.333 \n 3 0.736 0.264 \n 4 0.936 0.0643\n 5 0.935 0.0650\n 6 0.944 0.0561\n 7 0.616 0.384 \n 8 0.824 0.176 \n 9 0.969 0.0310\n10 0.934 0.0661\n# ℹ 76,093 more rows\n\nLogloss&lt;- function(real,prediction){\n  results&lt;- -1/length(real)*sum(real*log10(prediction)+(1-real)*log10(1-prediction))\n  return(results)\n}\n\ntest&lt;- function(real,pred){\n  results&lt;-sum(log10(pred))\n  return(results)\n}\n\n\ntest(rawData$Diabetes_binary,cart_TreePredict)\n\n[1] -Inf\n\nfinal_compar&lt;-data.frame(rbind(Logloss(rawData$Diabetes_binary,bayesGLMPredict),\n                               Logloss(rawData$Diabetes_binary,cart_TreePredict),\n                               Logloss(rawData$Diabetes_binary,rf_Pred)))|&gt;\n  mutate(BestModel = c(\"Bayes GLM\", \"Classification Tree\", \"Random Forest\"))\n\nnames(final_compar)&lt;- c(\"LogLoss\",\"Mehod\")\nfinal_compar\n\n    LogLoss               Mehod\n1 0.3431751           Bayes GLM\n2       NaN Classification Tree\n3 0.3174830       Random Forest\n\n# best model = Random Forest with 7 variables"
  },
  {
    "objectID": "EDA.html",
    "href": "EDA.html",
    "title": "EDA",
    "section": "",
    "text": "This project will use the “diabetes_binary_health_indicators_BRFSS2015” data set. According Kaggle.com this data set contains 253,680 observation where the response variable is whether the observed individual does or does not not have diabetes (0 or 1 respectively) or has pre-diabetes/diabetes (1). The original data set contains 22 variables most of which are categorical. We have limited the variables to those below:\n\nDiabetes_binary (response variable) - This variable illustrates if the subject has or does not have diabetes.\nHighBP (Categorical) - This is a binary variable describing whether or not the subject has high blood pressure.\nHighChol (Categorical) - This is a binary variable describing whether or not the subject has high cholesterol.\nSmoker (Categorical) - This is a binary variable describing whether or not the subject has smoked at least 100 cigarettes in their lifetime. 100 cigarettes is equal to 5 packs.\nHeartDiseaseorAttack (Categorical) - This is a binary variable describing whether or not the subject has coronary heart disease or myocardial infarction (a heart attack).\n\nPhysActivity- This is a binary variable describing whether or not the subject has conducted physical activity (outside of their job) in the past 30 days.\nHvyAlcoholConsump (Categorical) - This is a binary variable describing whether or not the subject, if male, consumes 14 or more alcoholic drinks per week, or if female, consumes 7 or more alcoholic drinks per week.\nMentHlth - This variable is a scale from 1 to 30 of how many days the subject has had POOR mental health.\nAge (Categorical) - This variable breaks age into 13 levels which are 18-24, 25-29, 30-34, 35-39, 40-44, 45-49, 50-54, 55-59, 60-64, 65-69, 70-74, 75-80, and 80 or older.\n\n\n\nThe purpose of my exploratory data analysis (EDA) was to identify variables that may have some significant relationship to the diabetes_binary variable. That is to say, does a particular variable either have a relationship to the response variable or not. I explored this by making 4 plots that helped illustrate the possible relationship. I also investigated variables that I did not ultimately use and added variables that I did not investigate initially. The ultimate goal of my model will be to determine what combination of the selected variables are accurate indicators of diabetes.\n\n\n\n\n   First we will load in the packages and the raw data. We then identified categorical variables and their levels by finding unique observations for each variable. Checking for missing values will come next. Finally, variables were converted to factors in order to better display information in the plots. Each plot has a title and a few other visualization adjustments to help the viewer understand.\n\nlibrary(tidyverse)\n\n\nrawData &lt;- read.csv(\"./diabetes_binary_health_indicators_BRFSS2015.csv\")\n\n# Quick determination of categorical variables\nfor (i in 1:length(rawData)) {\n  print(unique(rawData[i]))\n}\n\n  Diabetes_binary\n1               0\n9               1\n  HighBP\n1      1\n2      0\n  HighChol\n1        1\n2        0\n  CholCheck\n1         1\n2         0\n       BMI\n1       40\n2       25\n3       28\n4       27\n5       24\n7       30\n12      34\n13      26\n15      33\n17      21\n18      23\n21      22\n22      38\n26      32\n27      37\n30      31\n56      29\n60      20\n77      35\n86      45\n88      39\n102     19\n157     47\n160     18\n173     36\n189     43\n202     55\n204     49\n217     42\n222     17\n268     16\n271     41\n370     44\n659     50\n671     59\n744     48\n759     52\n783     46\n953     54\n1210    57\n1593    53\n1659    14\n1739    15\n1742    51\n2279    58\n2416    63\n2487    61\n4208    56\n5612    74\n5765    62\n6333    64\n8973    66\n10326   73\n11615   85\n13434   60\n14820   67\n15301   65\n17874   70\n18416   82\n19544   79\n19817   92\n25697   68\n31169   72\n33840   88\n36325   96\n38980   13\n41801   81\n47203   71\n47737   75\n49828   12\n59791   77\n65232   69\n72495   76\n76314   87\n76316   89\n76336   84\n76354   95\n76371   98\n79364   91\n79484   86\n79488   83\n157805  80\n165696  90\n205516  78\n  Smoker\n1      1\n3      0\n   Stroke\n1       0\n15      1\n  HeartDiseaseorAttack\n1                    0\n9                    1\n  PhysActivity\n1            0\n2            1\n  Fruits\n1      0\n3      1\n  Veggies\n1       1\n2       0\n   HvyAlcoholConsump\n1                  0\n20                 1\n  AnyHealthcare\n1             1\n2             0\n  NoDocbcCost\n1           0\n2           1\n   GenHlth\n1        5\n2        3\n4        2\n14       4\n24       1\n     MentHlth\n1          18\n2           0\n3          30\n5           3\n16          5\n19         15\n20         10\n25          6\n29         20\n33          2\n44         25\n48          1\n55          4\n87          7\n142         8\n155        21\n159        14\n173        26\n353        29\n370        16\n401        28\n471        11\n1072       12\n1480       24\n1482       17\n1629       13\n2534       27\n4789       19\n5351       22\n6576        9\n9608       23\n     PhysHlth\n1          15\n2           0\n3          30\n6           2\n7          14\n15         28\n23          7\n29         20\n37          3\n38         10\n39          1\n42          5\n51         17\n76          4\n111        19\n140         6\n219        12\n294        25\n323        27\n345        21\n353        22\n370         8\n383        29\n401        24\n566         9\n1029       16\n1327       18\n1544       23\n1629       13\n3689       26\n6293       11\n  DiffWalk\n1        1\n2        0\n  Sex\n1   0\n6   1\n   Age\n1    9\n2    7\n4   11\n6   10\n10   8\n11  13\n15   4\n16   6\n19   2\n21  12\n23   5\n75   1\n92   3\n    Education\n1           4\n2           6\n4           3\n5           5\n22          2\n557         1\n   Income\n1       3\n2       1\n3       8\n4       6\n5       4\n7       7\n15      2\n23      5\n\n\n\n#create a new data frame for cleaning and converts all binary variables into factors\nCleaning1&lt;- rawData|&gt;\n  drop_na()|&gt;\n  mutate(Diabetes_binary = as.factor(Diabetes_binary),\n         HighBP = as.factor(HighBP),\n         HighChol = as.factor(HighChol),\n         CholCheck = as.factor(CholCheck),\n         Smoker = as.factor(Smoker),\n         Stroke = as.factor(Stroke),\n         HeartDiseaseorAttack = as.factor(HeartDiseaseorAttack),\n         PhysActivity = as.factor(PhysActivity),\n         Fruits = as.factor(Fruits),\n         Veggies = as.factor(Veggies),\n         HvyAlcoholConsump = as.factor(HvyAlcoholConsump),\n         AnyHealthcare = as.factor(AnyHealthcare),\n         NoDocbcCost = as.factor(NoDocbcCost),\n         DiffWalk = as.factor(DiffWalk),\n         Sex = as.factor(Sex)\n         )\n\n\n\n\nIn this section I created 4 summary tables and 4 plots to help illustrate the relation between diabetes_binary and different variables. My initial selection of variables were body mass index (BMI), Mental Health (MentHlth), Heavy Alcohol Consumption (HvyAlcoholConsump), Age, Education, and Income. Ultimately my model used all of these variables except Education and Income.\n\n# frequency Body Mass Index (actual) by diabetes_binary\nBMIsummary&lt;- Cleaning1|&gt;\n  count(Diabetes_binary, BMI, name = \"Count\")\n\nprint(BMIsummary)\n\n    Diabetes_binary BMI Count\n1                 0  12     6\n2                 0  13    19\n3                 0  14    37\n4                 0  15   120\n5                 0  16   328\n6                 0  17   728\n7                 0  18  1720\n8                 0  19  3833\n9                 0  20  6086\n10                0  21  9376\n11                0  22 12952\n12                0  23 14697\n13                0  24 18081\n14                0  25 15695\n15                0  26 18560\n16                0  27 21849\n17                0  28 14294\n18                0  29 12659\n19                0  30 12251\n20                0  31 10163\n21                0  32  8354\n22                0  33  6908\n23                0  34  5494\n24                0  35  4131\n25                0  36  3393\n26                0  37  3029\n27                0  38  2385\n28                0  39  2056\n29                0  40  1534\n30                0  41  1141\n31                0  42  1127\n32                0  43   989\n33                0  44   697\n34                0  45   517\n35                0  46   471\n36                0  47   404\n37                0  48   304\n38                0  49   250\n39                0  50   224\n40                0  51   155\n41                0  52   129\n42                0  53   152\n43                0  54    65\n44                0  55   104\n45                0  56    66\n46                0  57    61\n47                0  58    36\n48                0  59    32\n49                0  60    38\n50                0  61    21\n51                0  62    27\n52                0  63    16\n53                0  64    16\n54                0  65     9\n55                0  66     4\n56                0  67     9\n57                0  68     8\n58                0  69     6\n59                0  70    10\n60                0  71    44\n61                0  72     7\n62                0  73    44\n63                0  74    15\n64                0  75    46\n65                0  76     3\n66                0  77    48\n67                0  79    62\n68                0  80     1\n69                0  81    42\n70                0  82    31\n71                0  83     1\n72                0  84    39\n73                0  86     1\n74                0  87    52\n75                0  88     2\n76                0  89    25\n77                0  90     1\n78                0  91     1\n79                0  92    27\n80                0  95    11\n81                0  96     1\n82                0  98     4\n83                1  13     2\n84                1  14     4\n85                1  15    12\n86                1  16    20\n87                1  17    48\n88                1  18    83\n89                1  19   135\n90                1  20   241\n91                1  21   479\n92                1  22   691\n93                1  23   913\n94                1  24  1469\n95                1  25  1451\n96                1  26  2002\n97                1  27  2757\n98                1  28  2251\n99                1  29  2231\n100               1  30  2322\n101               1  31  2112\n102               1  32  2120\n103               1  33  2040\n104               1  34  1687\n105               1  35  1444\n106               1  36  1240\n107               1  37  1118\n108               1  38  1012\n109               1  39   855\n110               1  40   724\n111               1  41   518\n112               1  42   512\n113               1  43   511\n114               1  44   346\n115               1  45   302\n116               1  46   279\n117               1  47   218\n118               1  48   180\n119               1  49   166\n120               1  50   148\n121               1  51    98\n122               1  52    86\n123               1  53    85\n124               1  54    48\n125               1  55    65\n126               1  56    43\n127               1  57    25\n128               1  58    35\n129               1  59    22\n130               1  60    25\n131               1  61    14\n132               1  62    16\n133               1  63    18\n134               1  64     8\n135               1  65    10\n136               1  66     9\n137               1  67     6\n138               1  68     6\n139               1  69     3\n140               1  70     5\n141               1  71     5\n142               1  72     7\n143               1  73     3\n144               1  74     1\n145               1  75     6\n146               1  77     7\n147               1  78     1\n148               1  79     4\n149               1  80     1\n150               1  81     7\n151               1  82     6\n152               1  83     1\n153               1  84     5\n154               1  85     1\n155               1  87     9\n156               1  89     3\n157               1  92     5\n158               1  95     1\n159               1  98     3\n\nggplot(BMIsummary,aes(x = BMI, y = Count)) +\n  labs(title = \"BMI Summary by diabetes\",\n       x = \"BMI Value\",\n       y = \"Frequecy\")+\n  theme(plot.title = element_text(hjust = .5))+\n  geom_bar(stat = \"identity\",position = \"dodge\", aes(fill = Diabetes_binary))+\n  facet_wrap(~Diabetes_binary) \n\n\n\n\n\n\n\nHealthSummary&lt;-Cleaning1|&gt;\n  count(Diabetes_binary,MentHlth)\n\nprint(HealthSummary)\n\n   Diabetes_binary MentHlth      n\n1                0        0 152277\n2                0        1   7726\n3                0        2  11546\n4                0        3   6457\n5                0        4   3300\n6                0        5   7807\n7                0        6    824\n8                0        7   2695\n9                0        8    529\n10               0        9     78\n11               0       10   5309\n12               0       11     38\n13               0       12    331\n14               0       13     33\n15               0       14    969\n16               0       15   4482\n17               0       16     74\n18               0       17     43\n19               0       18     77\n20               0       19     12\n21               0       20   2701\n22               0       21    179\n23               0       22     52\n24               0       23     30\n25               0       24     27\n26               0       25    915\n27               0       26     38\n28               0       27     67\n29               0       28    270\n30               0       29    128\n31               0       30   9320\n32               1        0  23403\n33               1        1    812\n34               1        2   1508\n35               1        3    924\n36               1        4    489\n37               1        5   1223\n38               1        6    164\n39               1        7    405\n40               1        8    110\n41               1        9     13\n42               1       10   1064\n43               1       11      3\n44               1       12     67\n45               1       13      8\n46               1       14    198\n47               1       15   1023\n48               1       16     14\n49               1       17     11\n50               1       18     20\n51               1       19      4\n52               1       20    663\n53               1       21     48\n54               1       22     11\n55               1       23      8\n56               1       24      6\n57               1       25    273\n58               1       26      7\n59               1       27     12\n60               1       28     57\n61               1       29     30\n62               1       30   2768\n\nggplot(HealthSummary, aes(MentHlth,n,colour = Diabetes_binary)) +\n  labs(title = \"Mental Health Summary by diabetes\",\n       x = \"Days of Poor Mental Health\",\n       y = \"Frequecy\")+\n  theme(plot.title = element_text(hjust = .5))+\n  geom_point(stat = \"identity\")+\n  geom_smooth(aes(MentHlth,n),stat = \"identity\")\n\n\n\n\n\n\n\n# Heavy Alcohol Consumpsion \n# Heavy drinkers (adult men having more than 14 drinks per week and adult women having more than 7 drinks per week)\n\nAlcoholSummary&lt;- Cleaning1|&gt;\n  group_by(Diabetes_binary,HvyAlcoholConsump)|&gt;\n  tally()\n\nAlcoholSummary\n\n# A tibble: 4 × 3\n# Groups:   Diabetes_binary [2]\n  Diabetes_binary HvyAlcoholConsump      n\n  &lt;fct&gt;           &lt;fct&gt;              &lt;int&gt;\n1 0               0                 204910\n2 0               1                  13424\n3 1               0                  34514\n4 1               1                    832\n\nggplot(Cleaning1,aes(PhysHlth,fill = Diabetes_binary)) +\n  geom_bar(position = \"dodge\") +\n  labs(title = \"Physical Health Summary by diabetes\",\n       x = \"Days where subject exercised outside of work\",\n       y = \"Frequecy\")+\n  theme(plot.title = element_text(hjust = .5))\n\n\n\n\n\n\n\nGenBox &lt;- Cleaning1|&gt;\n  group_by(Diabetes_binary)|&gt;\n  pivot_longer(col = c(Age,Education,Income),names_to = \"Category\", values_to = \"level\")|&gt;\n  count(Diabetes_binary,Category,level,name = \"val\")\n\nGenBox\n\n# A tibble: 54 × 4\n# Groups:   Diabetes_binary [2]\n   Diabetes_binary Category level   val\n   &lt;fct&gt;           &lt;chr&gt;    &lt;dbl&gt; &lt;int&gt;\n 1 0               Age          1  5622\n 2 0               Age          2  7458\n 3 0               Age          3 10809\n 4 0               Age          4 13197\n 5 0               Age          5 15106\n 6 0               Age          6 18077\n 7 0               Age          7 23226\n 8 0               Age          8 26569\n 9 0               Age          9 27511\n10 0               Age         10 25636\n# ℹ 44 more rows\n\nggplot(GenBox, aes(Diabetes_binary,level, fill = Diabetes_binary)) +\n  geom_boxplot()+\n  facet_wrap(~Category)+\n  scale_y_continuous(breaks = c(1:13))+\n  geom_text(aes(label=val))+\n  labs(title = \"Summary of Age, Education, and Income by diabetes\",\n       y = \"Levels (Values on Plot = Frequency)\")+\n  theme(plot.title = element_text(hjust = .5))\n\n\n\n\n\n\n\nMeans&lt;- rawData|&gt;\n  select(Diabetes_binary,HighBP,HighChol,HvyAlcoholConsump,Smoker,PhysActivity,Age,HeartDiseaseorAttack,MentHlth,BMI)|&gt;\n  summarise(Diabetes_binary.mean = round(mean(Diabetes_binary),digits = 2),\n            HighBP.mean = round(mean(HighBP),digits = 2),\n            HighChol.mean = round(mean(HighChol),digits = 2),\n            HvyAlcoholConsump.mean = round(mean(HvyAlcoholConsump),digits = 2),\n            Smoker.mean = round(mean(Smoker),digits = 2),\n            PhysActivity.mean = round(mean(PhysActivity),digits = 2),\n            AgeGroup.mean = round(mean(Age),digits = 2),\n            HeartDiseaseorAttack.mean = round(mean(HeartDiseaseorAttack),digits = 2),\n            MentHlth.mean = round(mean(MentHlth),digits = 2),\n            BMI.mean = round(mean(BMI),digits = 2))|&gt;\n  pivot_longer(cols = 1:10,names_to = \"Variable\")\n\nMeans\n\n# A tibble: 10 × 2\n   Variable                  value\n   &lt;chr&gt;                     &lt;dbl&gt;\n 1 Diabetes_binary.mean       0.14\n 2 HighBP.mean                0.43\n 3 HighChol.mean              0.42\n 4 HvyAlcoholConsump.mean     0.06\n 5 Smoker.mean                0.44\n 6 PhysActivity.mean          0.76\n 7 AgeGroup.mean              8.03\n 8 HeartDiseaseorAttack.mean  0.09\n 9 MentHlth.mean              3.18\n10 BMI.mean                  28.4 \n\nggplot(Means,aes(x=Variable,y=value))+\n  geom_bar(stat = \"identity\", position = \"dodge\")+ \n  labs(title = \"Average by Final Selected Variables\",\n       x = \"Variable\",\n       y = \"Mean\")+\n  theme(plot.title = element_text(hjust = .5))+\n  geom_text(aes(label = value),\n            vjust = -.5)+\n  theme(axis.text.x = element_text(angle = 15,\n                                   vjust = 1.25,\n                                   hjust = 1,\n                                   size = 8))\n\n\n\n\n\n\n\n\n```\nClick here for the Modeling Page"
  },
  {
    "objectID": "EDA.html#introduction-section",
    "href": "EDA.html#introduction-section",
    "title": "EDA",
    "section": "",
    "text": "This project will use the “diabetes_binary_health_indicators_BRFSS2015” data set. According Kaggle.com this data set contains 253,680 observation where the response variable is whether the observed individual does or does not not have diabetes (0 or 1 respectively) or has pre-diabetes/diabetes (1). The original data set contains 22 variables most of which are categorical. We have limited the variables to those below:\n\nDiabetes_binary (response variable) - This variable illustrates if the subject has or does not have diabetes.\nHighBP (Categorical) - This is a binary variable describing whether or not the subject has high blood pressure.\nHighChol (Categorical) - This is a binary variable describing whether or not the subject has high cholesterol.\nSmoker (Categorical) - This is a binary variable describing whether or not the subject has smoked at least 100 cigarettes in their lifetime. 100 cigarettes is equal to 5 packs.\nHeartDiseaseorAttack (Categorical) - This is a binary variable describing whether or not the subject has coronary heart disease or myocardial infarction (a heart attack).\n\nPhysActivity- This is a binary variable describing whether or not the subject has conducted physical activity (outside of their job) in the past 30 days.\nHvyAlcoholConsump (Categorical) - This is a binary variable describing whether or not the subject, if male, consumes 14 or more alcoholic drinks per week, or if female, consumes 7 or more alcoholic drinks per week.\nMentHlth - This variable is a scale from 1 to 30 of how many days the subject has had POOR mental health.\nAge (Categorical) - This variable breaks age into 13 levels which are 18-24, 25-29, 30-34, 35-39, 40-44, 45-49, 50-54, 55-59, 60-64, 65-69, 70-74, 75-80, and 80 or older.\n\n\n\nThe purpose of my exploratory data analysis (EDA) was to identify variables that may have some significant relationship to the diabetes_binary variable. That is to say, does a particular variable either have a relationship to the response variable or not. I explored this by making 4 plots that helped illustrate the possible relationship. I also investigated variables that I did not ultimately use and added variables that I did not investigate initially. The ultimate goal of my model will be to determine what combination of the selected variables are accurate indicators of diabetes."
  },
  {
    "objectID": "EDA.html#data",
    "href": "EDA.html#data",
    "title": "EDA",
    "section": "",
    "text": "First we will load in the packages and the raw data. We then identified categorical variables and their levels by finding unique observations for each variable. Checking for missing values will come next. Finally, variables were converted to factors in order to better display information in the plots. Each plot has a title and a few other visualization adjustments to help the viewer understand.\n\nlibrary(tidyverse)\n\n\nrawData &lt;- read.csv(\"./diabetes_binary_health_indicators_BRFSS2015.csv\")\n\n# Quick determination of categorical variables\nfor (i in 1:length(rawData)) {\n  print(unique(rawData[i]))\n}\n\n  Diabetes_binary\n1               0\n9               1\n  HighBP\n1      1\n2      0\n  HighChol\n1        1\n2        0\n  CholCheck\n1         1\n2         0\n       BMI\n1       40\n2       25\n3       28\n4       27\n5       24\n7       30\n12      34\n13      26\n15      33\n17      21\n18      23\n21      22\n22      38\n26      32\n27      37\n30      31\n56      29\n60      20\n77      35\n86      45\n88      39\n102     19\n157     47\n160     18\n173     36\n189     43\n202     55\n204     49\n217     42\n222     17\n268     16\n271     41\n370     44\n659     50\n671     59\n744     48\n759     52\n783     46\n953     54\n1210    57\n1593    53\n1659    14\n1739    15\n1742    51\n2279    58\n2416    63\n2487    61\n4208    56\n5612    74\n5765    62\n6333    64\n8973    66\n10326   73\n11615   85\n13434   60\n14820   67\n15301   65\n17874   70\n18416   82\n19544   79\n19817   92\n25697   68\n31169   72\n33840   88\n36325   96\n38980   13\n41801   81\n47203   71\n47737   75\n49828   12\n59791   77\n65232   69\n72495   76\n76314   87\n76316   89\n76336   84\n76354   95\n76371   98\n79364   91\n79484   86\n79488   83\n157805  80\n165696  90\n205516  78\n  Smoker\n1      1\n3      0\n   Stroke\n1       0\n15      1\n  HeartDiseaseorAttack\n1                    0\n9                    1\n  PhysActivity\n1            0\n2            1\n  Fruits\n1      0\n3      1\n  Veggies\n1       1\n2       0\n   HvyAlcoholConsump\n1                  0\n20                 1\n  AnyHealthcare\n1             1\n2             0\n  NoDocbcCost\n1           0\n2           1\n   GenHlth\n1        5\n2        3\n4        2\n14       4\n24       1\n     MentHlth\n1          18\n2           0\n3          30\n5           3\n16          5\n19         15\n20         10\n25          6\n29         20\n33          2\n44         25\n48          1\n55          4\n87          7\n142         8\n155        21\n159        14\n173        26\n353        29\n370        16\n401        28\n471        11\n1072       12\n1480       24\n1482       17\n1629       13\n2534       27\n4789       19\n5351       22\n6576        9\n9608       23\n     PhysHlth\n1          15\n2           0\n3          30\n6           2\n7          14\n15         28\n23          7\n29         20\n37          3\n38         10\n39          1\n42          5\n51         17\n76          4\n111        19\n140         6\n219        12\n294        25\n323        27\n345        21\n353        22\n370         8\n383        29\n401        24\n566         9\n1029       16\n1327       18\n1544       23\n1629       13\n3689       26\n6293       11\n  DiffWalk\n1        1\n2        0\n  Sex\n1   0\n6   1\n   Age\n1    9\n2    7\n4   11\n6   10\n10   8\n11  13\n15   4\n16   6\n19   2\n21  12\n23   5\n75   1\n92   3\n    Education\n1           4\n2           6\n4           3\n5           5\n22          2\n557         1\n   Income\n1       3\n2       1\n3       8\n4       6\n5       4\n7       7\n15      2\n23      5\n\n\n\n#create a new data frame for cleaning and converts all binary variables into factors\nCleaning1&lt;- rawData|&gt;\n  drop_na()|&gt;\n  mutate(Diabetes_binary = as.factor(Diabetes_binary),\n         HighBP = as.factor(HighBP),\n         HighChol = as.factor(HighChol),\n         CholCheck = as.factor(CholCheck),\n         Smoker = as.factor(Smoker),\n         Stroke = as.factor(Stroke),\n         HeartDiseaseorAttack = as.factor(HeartDiseaseorAttack),\n         PhysActivity = as.factor(PhysActivity),\n         Fruits = as.factor(Fruits),\n         Veggies = as.factor(Veggies),\n         HvyAlcoholConsump = as.factor(HvyAlcoholConsump),\n         AnyHealthcare = as.factor(AnyHealthcare),\n         NoDocbcCost = as.factor(NoDocbcCost),\n         DiffWalk = as.factor(DiffWalk),\n         Sex = as.factor(Sex)\n         )"
  },
  {
    "objectID": "EDA.html#summarizations",
    "href": "EDA.html#summarizations",
    "title": "EDA",
    "section": "",
    "text": "In this section I created 4 summary tables and 4 plots to help illustrate the relation between diabetes_binary and different variables. My initial selection of variables were body mass index (BMI), Mental Health (MentHlth), Heavy Alcohol Consumption (HvyAlcoholConsump), Age, Education, and Income. Ultimately my model used all of these variables except Education and Income.\n\n# frequency Body Mass Index (actual) by diabetes_binary\nBMIsummary&lt;- Cleaning1|&gt;\n  count(Diabetes_binary, BMI, name = \"Count\")\n\nprint(BMIsummary)\n\n    Diabetes_binary BMI Count\n1                 0  12     6\n2                 0  13    19\n3                 0  14    37\n4                 0  15   120\n5                 0  16   328\n6                 0  17   728\n7                 0  18  1720\n8                 0  19  3833\n9                 0  20  6086\n10                0  21  9376\n11                0  22 12952\n12                0  23 14697\n13                0  24 18081\n14                0  25 15695\n15                0  26 18560\n16                0  27 21849\n17                0  28 14294\n18                0  29 12659\n19                0  30 12251\n20                0  31 10163\n21                0  32  8354\n22                0  33  6908\n23                0  34  5494\n24                0  35  4131\n25                0  36  3393\n26                0  37  3029\n27                0  38  2385\n28                0  39  2056\n29                0  40  1534\n30                0  41  1141\n31                0  42  1127\n32                0  43   989\n33                0  44   697\n34                0  45   517\n35                0  46   471\n36                0  47   404\n37                0  48   304\n38                0  49   250\n39                0  50   224\n40                0  51   155\n41                0  52   129\n42                0  53   152\n43                0  54    65\n44                0  55   104\n45                0  56    66\n46                0  57    61\n47                0  58    36\n48                0  59    32\n49                0  60    38\n50                0  61    21\n51                0  62    27\n52                0  63    16\n53                0  64    16\n54                0  65     9\n55                0  66     4\n56                0  67     9\n57                0  68     8\n58                0  69     6\n59                0  70    10\n60                0  71    44\n61                0  72     7\n62                0  73    44\n63                0  74    15\n64                0  75    46\n65                0  76     3\n66                0  77    48\n67                0  79    62\n68                0  80     1\n69                0  81    42\n70                0  82    31\n71                0  83     1\n72                0  84    39\n73                0  86     1\n74                0  87    52\n75                0  88     2\n76                0  89    25\n77                0  90     1\n78                0  91     1\n79                0  92    27\n80                0  95    11\n81                0  96     1\n82                0  98     4\n83                1  13     2\n84                1  14     4\n85                1  15    12\n86                1  16    20\n87                1  17    48\n88                1  18    83\n89                1  19   135\n90                1  20   241\n91                1  21   479\n92                1  22   691\n93                1  23   913\n94                1  24  1469\n95                1  25  1451\n96                1  26  2002\n97                1  27  2757\n98                1  28  2251\n99                1  29  2231\n100               1  30  2322\n101               1  31  2112\n102               1  32  2120\n103               1  33  2040\n104               1  34  1687\n105               1  35  1444\n106               1  36  1240\n107               1  37  1118\n108               1  38  1012\n109               1  39   855\n110               1  40   724\n111               1  41   518\n112               1  42   512\n113               1  43   511\n114               1  44   346\n115               1  45   302\n116               1  46   279\n117               1  47   218\n118               1  48   180\n119               1  49   166\n120               1  50   148\n121               1  51    98\n122               1  52    86\n123               1  53    85\n124               1  54    48\n125               1  55    65\n126               1  56    43\n127               1  57    25\n128               1  58    35\n129               1  59    22\n130               1  60    25\n131               1  61    14\n132               1  62    16\n133               1  63    18\n134               1  64     8\n135               1  65    10\n136               1  66     9\n137               1  67     6\n138               1  68     6\n139               1  69     3\n140               1  70     5\n141               1  71     5\n142               1  72     7\n143               1  73     3\n144               1  74     1\n145               1  75     6\n146               1  77     7\n147               1  78     1\n148               1  79     4\n149               1  80     1\n150               1  81     7\n151               1  82     6\n152               1  83     1\n153               1  84     5\n154               1  85     1\n155               1  87     9\n156               1  89     3\n157               1  92     5\n158               1  95     1\n159               1  98     3\n\nggplot(BMIsummary,aes(x = BMI, y = Count)) +\n  labs(title = \"BMI Summary by diabetes\",\n       x = \"BMI Value\",\n       y = \"Frequecy\")+\n  theme(plot.title = element_text(hjust = .5))+\n  geom_bar(stat = \"identity\",position = \"dodge\", aes(fill = Diabetes_binary))+\n  facet_wrap(~Diabetes_binary) \n\n\n\n\n\n\n\nHealthSummary&lt;-Cleaning1|&gt;\n  count(Diabetes_binary,MentHlth)\n\nprint(HealthSummary)\n\n   Diabetes_binary MentHlth      n\n1                0        0 152277\n2                0        1   7726\n3                0        2  11546\n4                0        3   6457\n5                0        4   3300\n6                0        5   7807\n7                0        6    824\n8                0        7   2695\n9                0        8    529\n10               0        9     78\n11               0       10   5309\n12               0       11     38\n13               0       12    331\n14               0       13     33\n15               0       14    969\n16               0       15   4482\n17               0       16     74\n18               0       17     43\n19               0       18     77\n20               0       19     12\n21               0       20   2701\n22               0       21    179\n23               0       22     52\n24               0       23     30\n25               0       24     27\n26               0       25    915\n27               0       26     38\n28               0       27     67\n29               0       28    270\n30               0       29    128\n31               0       30   9320\n32               1        0  23403\n33               1        1    812\n34               1        2   1508\n35               1        3    924\n36               1        4    489\n37               1        5   1223\n38               1        6    164\n39               1        7    405\n40               1        8    110\n41               1        9     13\n42               1       10   1064\n43               1       11      3\n44               1       12     67\n45               1       13      8\n46               1       14    198\n47               1       15   1023\n48               1       16     14\n49               1       17     11\n50               1       18     20\n51               1       19      4\n52               1       20    663\n53               1       21     48\n54               1       22     11\n55               1       23      8\n56               1       24      6\n57               1       25    273\n58               1       26      7\n59               1       27     12\n60               1       28     57\n61               1       29     30\n62               1       30   2768\n\nggplot(HealthSummary, aes(MentHlth,n,colour = Diabetes_binary)) +\n  labs(title = \"Mental Health Summary by diabetes\",\n       x = \"Days of Poor Mental Health\",\n       y = \"Frequecy\")+\n  theme(plot.title = element_text(hjust = .5))+\n  geom_point(stat = \"identity\")+\n  geom_smooth(aes(MentHlth,n),stat = \"identity\")\n\n\n\n\n\n\n\n# Heavy Alcohol Consumpsion \n# Heavy drinkers (adult men having more than 14 drinks per week and adult women having more than 7 drinks per week)\n\nAlcoholSummary&lt;- Cleaning1|&gt;\n  group_by(Diabetes_binary,HvyAlcoholConsump)|&gt;\n  tally()\n\nAlcoholSummary\n\n# A tibble: 4 × 3\n# Groups:   Diabetes_binary [2]\n  Diabetes_binary HvyAlcoholConsump      n\n  &lt;fct&gt;           &lt;fct&gt;              &lt;int&gt;\n1 0               0                 204910\n2 0               1                  13424\n3 1               0                  34514\n4 1               1                    832\n\nggplot(Cleaning1,aes(PhysHlth,fill = Diabetes_binary)) +\n  geom_bar(position = \"dodge\") +\n  labs(title = \"Physical Health Summary by diabetes\",\n       x = \"Days where subject exercised outside of work\",\n       y = \"Frequecy\")+\n  theme(plot.title = element_text(hjust = .5))\n\n\n\n\n\n\n\nGenBox &lt;- Cleaning1|&gt;\n  group_by(Diabetes_binary)|&gt;\n  pivot_longer(col = c(Age,Education,Income),names_to = \"Category\", values_to = \"level\")|&gt;\n  count(Diabetes_binary,Category,level,name = \"val\")\n\nGenBox\n\n# A tibble: 54 × 4\n# Groups:   Diabetes_binary [2]\n   Diabetes_binary Category level   val\n   &lt;fct&gt;           &lt;chr&gt;    &lt;dbl&gt; &lt;int&gt;\n 1 0               Age          1  5622\n 2 0               Age          2  7458\n 3 0               Age          3 10809\n 4 0               Age          4 13197\n 5 0               Age          5 15106\n 6 0               Age          6 18077\n 7 0               Age          7 23226\n 8 0               Age          8 26569\n 9 0               Age          9 27511\n10 0               Age         10 25636\n# ℹ 44 more rows\n\nggplot(GenBox, aes(Diabetes_binary,level, fill = Diabetes_binary)) +\n  geom_boxplot()+\n  facet_wrap(~Category)+\n  scale_y_continuous(breaks = c(1:13))+\n  geom_text(aes(label=val))+\n  labs(title = \"Summary of Age, Education, and Income by diabetes\",\n       y = \"Levels (Values on Plot = Frequency)\")+\n  theme(plot.title = element_text(hjust = .5))\n\n\n\n\n\n\n\nMeans&lt;- rawData|&gt;\n  select(Diabetes_binary,HighBP,HighChol,HvyAlcoholConsump,Smoker,PhysActivity,Age,HeartDiseaseorAttack,MentHlth,BMI)|&gt;\n  summarise(Diabetes_binary.mean = round(mean(Diabetes_binary),digits = 2),\n            HighBP.mean = round(mean(HighBP),digits = 2),\n            HighChol.mean = round(mean(HighChol),digits = 2),\n            HvyAlcoholConsump.mean = round(mean(HvyAlcoholConsump),digits = 2),\n            Smoker.mean = round(mean(Smoker),digits = 2),\n            PhysActivity.mean = round(mean(PhysActivity),digits = 2),\n            AgeGroup.mean = round(mean(Age),digits = 2),\n            HeartDiseaseorAttack.mean = round(mean(HeartDiseaseorAttack),digits = 2),\n            MentHlth.mean = round(mean(MentHlth),digits = 2),\n            BMI.mean = round(mean(BMI),digits = 2))|&gt;\n  pivot_longer(cols = 1:10,names_to = \"Variable\")\n\nMeans\n\n# A tibble: 10 × 2\n   Variable                  value\n   &lt;chr&gt;                     &lt;dbl&gt;\n 1 Diabetes_binary.mean       0.14\n 2 HighBP.mean                0.43\n 3 HighChol.mean              0.42\n 4 HvyAlcoholConsump.mean     0.06\n 5 Smoker.mean                0.44\n 6 PhysActivity.mean          0.76\n 7 AgeGroup.mean              8.03\n 8 HeartDiseaseorAttack.mean  0.09\n 9 MentHlth.mean              3.18\n10 BMI.mean                  28.4 \n\nggplot(Means,aes(x=Variable,y=value))+\n  geom_bar(stat = \"identity\", position = \"dodge\")+ \n  labs(title = \"Average by Final Selected Variables\",\n       x = \"Variable\",\n       y = \"Mean\")+\n  theme(plot.title = element_text(hjust = .5))+\n  geom_text(aes(label = value),\n            vjust = -.5)+\n  theme(axis.text.x = element_text(angle = 15,\n                                   vjust = 1.25,\n                                   hjust = 1,\n                                   size = 8))\n\n\n\n\n\n\n\n\n```\nClick here for the Modeling Page"
  }
]